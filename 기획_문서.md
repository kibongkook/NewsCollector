# ì–‘ì§ˆì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ í”„ë¡œê·¸ë¨ - ì™„ì„± ê¸°íšì„œ (v1.1)

**í”„ë¡œì íŠ¸ëª…**: NewsCollector (ì‚¬ìš©ì ë§ì¶¤í˜• ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ)  
**ë²„ì „**: v1.1  
**ì‘ì„±ì¼**: 2026-02-05  
**ìµœì¢… ëª©í‘œ**: ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•˜ì—¬, ì¼ë³„/ê¸°ê°„ë³„/Top-N í˜•íƒœë¡œ ì œê³µ

---

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ëª©í‘œ](#í”„ë¡œì íŠ¸-ëª©í‘œ)
2. [ì‹œìŠ¤í…œ êµ¬ì„± (9ê°œ í•µì‹¬ ëª¨ë“ˆ)](#ì‹œìŠ¤í…œ-êµ¬ì„±-9ê°œ-í•µì‹¬-ëª¨ë“ˆ)
3. [ëª¨ë“ˆë³„ ìƒì„¸ ì„¤ê³„](#ëª¨ë“ˆë³„-ìƒì„¸-ì„¤ê³„)
4. [ë°ì´í„° ëª¨ë¸ ì„¤ê³„](#ë°ì´í„°-ëª¨ë¸-ì„¤ê³„)
5. [ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹ ê³µì‹](#ì ìˆ˜-ê³„ì‚°-ë°-ë­í‚¹-ê³µì‹)
6. [API ë° ì¸í„°í˜ì´ìŠ¤](#api-ë°-ì¸í„°í˜ì´ìŠ¤)
7. [êµ¬í˜„ ì „ëµ ë° ìˆœì„œ](#êµ¬í˜„-ì „ëµ-ë°-ìˆœì„œ)
8. [ì„±ëŠ¥ ë° í™•ì¥ì„±](#ì„±ëŠ¥-ë°-í™•ì¥ì„±)

---

## í”„ë¡œì íŠ¸ ëª©í‘œ

### A. ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ
âœ… **ì •í™•í•œ ìš”ì²­ ì´í•´**: ìì—°ì–´/íŒŒë¼ë¯¸í„° ì…ë ¥ â†’ êµ¬ì¡°í™”ëœ QuerySpec  
âœ… **ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•©**: RSS, API, ì›¹ í¬ë¡¤ë§ ë“± ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘  
âœ… **í’ˆì§ˆ ë³´ì¦**: ì‹ ë¢°ë„, ë¬´ê²°ì„±, ì ì ˆì„± ê²€ì¦ì„ í†µê³¼í•œ ë‰´ìŠ¤ë§Œ ì œê³µ  
âœ… **ë‹¤ì–‘í•œ ì¡°íšŒ**: ì¼ë³„, ê¸°ê°„ë³„, ì¹´í…Œê³ ë¦¬ë³„, ì¸ê¸°ìˆœ, Top-N ì œê³µ  

### B. ê¸°ìˆ  ëª©í‘œ
âœ… **AI ìµœì†Œí™”**: ê·œì¹™ ê¸°ë°˜, í†µê³„ ê¸°ë°˜ ì ‘ê·¼ (NLP ìµœì†Œí™”)  
âœ… **í•˜ë“œì½”ë”© ì œê±°**: ëª¨ë“  ì„¤ì •ì„ YAML/JSONìœ¼ë¡œ ê´€ë¦¬  
âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì†ŒìŠ¤, í•„í„°, ì ìˆ˜ ëª¨ë¸ ì¶”ê°€ ìš©ì´  
âœ… **ê³ ì„±ëŠ¥**: 1000ê°œ ë‰´ìŠ¤ ì¡°íšŒ < 1ì´ˆ, ë³‘ë ¬ ìˆ˜ì§‘ ì²˜ë¦¬  

---

## ì‹œìŠ¤í…œ êµ¬ì„± (9ê°œ í•µì‹¬ ëª¨ë“ˆ)

### ì „ì²´ ë°ì´í„° íë¦„

```
ì‚¬ìš©ì ì…ë ¥ (ìì—°ì–´/íŒŒë¼ë¯¸í„°)
        â†“
1ï¸âƒ£  Request Parser (ì˜ë„ ì´í•´)
        â†“
    QuerySpec ìƒì„±
        â†“
2ï¸âƒ£  Source Registry (ì†ŒìŠ¤ ì¡°íšŒ)
        â†“
3ï¸âƒ£  Ingestion (ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘) [ë³‘ë ¬ ì²˜ë¦¬]
        â†“
    Raw News Data (ì›ë³¸)
        â†“
4ï¸âƒ£  Parsing & Normalization (ì •ê·œí™”)
        â†“
    Normalized News
        â†“
5ï¸âƒ£  Dedup & Clustering (ì¤‘ë³µ ì œê±°)
        â†“
    Unique News
        â†“
6ï¸âƒ£  Content Integrity QA (ë¬´ê²°ì„± ê²€ì¦) â­
        â†“
    IntegrityScore + Flags
        â†“
7ï¸âƒ£  Credibility & Quality Scoring (ì‹ ë¢°ë„/í’ˆì§ˆ) â­
        â†“
    CredibilityScore, QualityScore
        â†“
8ï¸âƒ£  Popularity Engine (ì¸ê¸°ë„)
        â†“
    PopularityScore
        â†“
9ï¸âƒ£  Ranker & Policy Filter (ìµœì¢… ì •ì±… í•„í„° + ë­í‚¹)
        â†“
ìµœì¢… ê²°ê³¼ (ì •ë ¬, ë‹¤ì–‘ì„± ë³´ì¥)
        â†“
ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ (JSON/CLI)
```

---

## ëª¨ë“ˆë³„ ìƒì„¸ ì„¤ê³„

---

### ëª¨ë“ˆ 1ï¸âƒ£: Request Parser (ì˜ë„ ì´í•´ ëª¨ë“ˆ)

**ì—­í• **: ì‚¬ìš©ì ì…ë ¥(ìì—°ì–´ ë˜ëŠ” íŒŒë¼ë¯¸í„°)ì„ í•´ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ QuerySpec ìƒì„±

#### ì…ë ¥ í˜•ì‹

```python
# ìì—°ì–´ ì˜ˆì‹œ
"2ì›” 1ì¼~5ì¼ ë™ì•ˆ AI ê´€ë ¨ ë‰´ìŠ¤ ì¤‘ ê²½ì œ ì¹´í…Œê³ ë¦¬ë§Œ ë³´ì—¬ì¤˜"
"ì–´ì œ ì •ì¹˜ ë‰´ìŠ¤ Top 10"
"ì§€ë‚œ 1ì£¼ì¼ê°„ ê°€ì¥ ë§ì´ ë³¸ ê³¼í•™ ë‰´ìŠ¤"

# íŒŒë¼ë¯¸í„° ì˜ˆì‹œ
{
  "date_from": "2026-02-01",
  "date_to": "2026-02-05",
  "keywords": ["AI", "ê¸°ìˆ "],
  "category": "ê²½ì œ",
  "limit": 20,
  "sort_by": "quality"
}
```

#### ì¶œë ¥: QuerySpec ê°ì²´

```python
@dataclass
class QuerySpec:
    # ì‹œê°„ ë²”ìœ„
    date_from: Optional[datetime]          # ê²€ìƒ‰ ì‹œì‘ì¼
    date_to: Optional[datetime]            # ê²€ìƒ‰ ì¢…ë£Œì¼
    
    # ìœ„ì¹˜/ì–¸ì–´ ì„¤ì •
    locale: str                            # ì˜ˆ: "ko_KR", "en_US"
    timezone: str                          # ì˜ˆ: "Asia/Seoul"
    country: str                           # ì˜ˆ: "KR" (ISO 3166-1)
    language: str                          # ì˜ˆ: "ko", "en"
    market: str                            # ì§€ì—­ + ì–¸ì–´ ì¡°í•©
    
    # ì½˜í…ì¸  í•„í„°
    category: Optional[List[str]]          # ì˜ˆ: ["ì •ì¹˜", "ê²½ì œ", "IT"]
    keywords: Optional[List[str]]          # í¬í•¨ í‚¤ì›Œë“œ
    exclude_keywords: Optional[List[str]]  # ì œì™¸ í‚¤ì›Œë“œ
    
    # ì¸ê¸°ë„/ìˆœì„œ
    popularity_type: str                   # "trending", "popular", "latest", "quality"
    group_by: str                          # "day", "source", "none"
    
    # í˜ì´ì§•
    limit: int                             # ê²°ê³¼ ê°œìˆ˜
    offset: int                            # í˜ì´ì§• ì˜¤í”„ì…‹
    
    # ì¶”ê°€ ì˜µì…˜
    verified_sources_only: bool            # Tier1 ì†ŒìŠ¤ë§Œ ì‚¬ìš©
    diversity: bool                        # ì†ŒìŠ¤ ë‹¤ì–‘ì„± ë³´ì¥
```

#### íŒŒì‹± ë¡œì§

```yaml
# natural_language_mapping.yaml
intent_patterns:
  "trending":
    keywords: ["íŠ¸ë Œë”©", "ì¸ê¸°", "í™”ì œ", "í•«", "ëœ¨ëŠ”"]
    result: {popularity_type: "trending", group_by: "day"}
  
  "popular":
    keywords: ["ë§ì´ë³¸", "ì¡°íšŒ", "ê³µìœ ", "ì¸ê¸°"]
    result: {popularity_type: "popular"}
  
  "latest":
    keywords: ["ìµœì‹ ", "ì–´ì œ", "ì˜¤ëŠ˜", "ì§€ë‚œ"]
    result: {popularity_type: "latest", sort_by: "published_at"}
  
  "quality":
    keywords: ["ì¢‹ì€", "ì‹ ë¢°", "í’ˆì§ˆ", "ì–‘ì§ˆ"]
    result: {popularity_type: "quality", sort_by: "quality_score"}

date_patterns:
  "ì–´ì œ": {offset_days: -1, range_days: 1}
  "ì§€ë‚œ 1ì£¼ì¼": {offset_days: -7, range_days: 7}
  "2ì›” 1ì¼~5ì¼": {date_from: "2026-02-01", date_to: "2026-02-05"}
```

---

### ëª¨ë“ˆ 2ï¸âƒ£: Source Registry (ì†ŒìŠ¤ DB/ë ˆì§€ìŠ¤íŠ¸ë¦¬)

**ì—­í• **: ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ì˜ ë©”íƒ€ë°ì´í„° ë° ìˆ˜ì§‘ ì •ì±… ì¤‘ì•™ ê´€ë¦¬

#### Source Entity êµ¬ì¡°

```python
@dataclass
class NewsSource:
    # ì‹ë³„ì
    id: str                               # ì˜ˆ: "naver_news_api"
    name: str                             # ì˜ˆ: "Naver News"
    
    # ìˆ˜ì§‘ ë°©ì‹
    ingestion_type: str                   # "api", "rss", "web_crawl"
    base_url: str                         # API ì—”ë“œí¬ì¸íŠ¸ ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸
    
    # ê¸°ë³¸ ë¡œì¼€ì¼ ì„¤ì •
    default_locale: str                   # "ko_KR"
    default_timezone: str                 # "Asia/Seoul"
    supported_locales: List[str]
    supported_categories: List[str]
    
    # ì‹ ë¢°ë„ ë“±ê¸‰
    tier: str                             # "whitelist" | "tier1" | "tier2" | "tier3" | "blacklist"
    tier_weights: {
        "whitelist": 1.0,
        "tier1": 0.95,
        "tier2": 0.80,
        "tier3": 0.60,
        "blacklist": 0.0
    }
    
    # Rate Limiting & ìºì‹œ
    rate_limit: {
        "requests_per_minute": 60,
        "requests_per_hour": 1000,
        "daily_quota": 10000
    }
    cache_ttl_minutes: int                # ê²°ê³¼ ìºì‹œ ìœ ì§€ ì‹œê°„
    
    # Robots.txt & í¬ë¡¤ë§ ì •ì±…
    respect_robots_txt: bool
    crawl_delay_seconds: int
    user_agent: str
    
    # ë©”íƒ€ë°ì´í„° ì œê³µ ì—¬ë¶€
    provides_metadata: {
        "author": bool,
        "views": bool,
        "shares": bool,
        "comments": bool,
        "publish_date": bool
    }
    
    # ìƒíƒœ
    is_active: bool
    last_crawled: Optional[datetime]
    last_success: Optional[datetime]
    failure_count: int
    
    # ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ (ë™ì )
    credibility_base_score: float         # 0~100 (Tierì— ë”°ë¼ ì´ˆê¸° ì„¤ì •)
```

#### Source Registry ì„¤ì • ì˜ˆì‹œ

```yaml
# sources_registry.yaml

sources:
  # Tier 1: ê²€ì¦ëœ ëŒ€í˜• ì–¸ë¡ ì‚¬
  naver_news:
    id: "naver_news"
    name: "Naver News API"
    tier: "tier1"
    ingestion_type: "api"
    base_url: "https://api.naver.com/v1/news"
    default_locale: "ko_KR"
    default_timezone: "Asia/Seoul"
    credibility_base_score: 88
    rate_limit:
      requests_per_minute: 100
    cache_ttl_minutes: 30
  
  google_news:
    id: "google_news"
    name: "Google News"
    tier: "tier1"
    ingestion_type: "rss"
    base_url: "https://feeds.google.com/news"
    default_locale: "en_US"
    credibility_base_score: 90
    cache_ttl_minutes: 20
  
  chosun_rss:
    id: "chosun_rss"
    name: "ì¡°ì„ ì¼ë³´ RSS"
    tier: "tier1"
    ingestion_type: "rss"
    base_url: "http://rss.chosun.com"
    default_locale: "ko_KR"
    credibility_base_score: 87
  
  unknown_blog:
    id: "unknown_blog"
    name: "Unknown Tech Blog"
    tier: "tier3"
    ingestion_type: "web_crawl"
    base_url: "https://unknownblog.example.com"
    credibility_base_score: 45
  
  spam_site:
    id: "spam_site"
    name: "Spam Content"
    tier: "blacklist"
    is_active: false
    credibility_base_score: 0

# Tier ì •ì˜
tier_definitions:
  whitelist:
    description: "ê³µì‹ ê¸°ê´€ ë°œí‘œ, ì •ë¶€, ê³µì‹ ì±„ë„"
    base_credibility: 95
    weight: 1.0
  
  tier1:
    description: "ì£¼ìš” ì‹ ë¬¸ì‚¬, ê²€ì¦ëœ ëŒ€í˜• ì–¸ë¡ ì‚¬"
    base_credibility: 88
    weight: 0.95
  
  tier2:
    description: "ì§€ì—­ ì–¸ë¡ ì‚¬, ì¤‘ì†Œ ë§¤ì²´"
    base_credibility: 70
    weight: 0.80
  
  tier3:
    description: "ê°œì¸ ë¸”ë¡œê·¸, ì†Œê·œëª¨ ì‚¬ì´íŠ¸"
    base_credibility: 45
    weight: 0.60
  
  blacklist:
    description: "ìŠ¤íŒ¸, ê±°ì§“ ë‰´ìŠ¤, ì•…ì„± ì‚¬ì´íŠ¸"
    base_credibility: 0
    weight: 0.0
```

---

### ëª¨ë“ˆ 3ï¸âƒ£: Ingestion (ìˆ˜ì§‘ ì»¤ë„¥í„°)

**ì—­í• **: Source Registry ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘

#### Ingestion Flow

```
QuerySpec + Source List
        â†“
ì†ŒìŠ¤ë³„ ì»¤ë„¥í„° ì„ íƒ (API/RSS/Crawl)
        â†“
Rate Limit í™•ì¸
        â†“
ë³‘ë ¬ ìš”ì²­ (async)
        â†“
Raw Data ì¶”ì¶œ
        â†“
ì›ë³¸ HTML + ë©”íƒ€ë°ì´í„° ì €ì¥
        â†“
Raw News Record ìƒì„±
```

#### Raw News Record (ì›ë³¸ ì €ì¥)

```python
@dataclass
class RawNewsRecord:
    id: str                              # MD5(source_id + url + published_at)
    source_id: str
    source_name: str
    
    # ì›ë³¸ ì €ì¥
    raw_html: str                        # ì›ë³¸ HTML ì „ì²´
    raw_data: dict                       # API ì‘ë‹µ JSON
    extracted_text: str                  # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    
    # ë©”íƒ€ë°ì´í„°
    url: str
    fetch_timestamp: datetime
    page_charset: str                    # "utf-8"
    page_language: str
    
    # Crawling ë©”íƒ€
    http_status: int
    response_time_ms: int
```

#### ì»¤ë„¥í„° êµ¬í˜„ ì˜ˆì‹œ

```python
# API ì»¤ë„¥í„°
class NaverNewsConnector(BaseConnector):
    async def fetch(self, query_spec: QuerySpec) -> List[RawNewsRecord]:
        """
        Naver News API í˜¸ì¶œ
        """
        url = "https://api.naver.com/v1/news/search"
        params = {
            "query": " ".join(query_spec.keywords),
            "sort": "date",  # ë˜ëŠ” "sim" (ìœ ì‚¬ë„)
            "start": query_spec.offset,
            "display": query_spec.limit
        }
        
        response = await self.http_client.get(url, params=params)
        raw_records = []
        
        for item in response.json()["items"]:
            record = RawNewsRecord(
                source_id="naver_news",
                source_name="Naver News",
                raw_data=item,
                url=item["link"],
                fetch_timestamp=datetime.now()
            )
            raw_records.append(record)
        
        return raw_records

# RSS ì»¤ë„¥í„°
class RSSConnector(BaseConnector):
    async def fetch(self, query_spec: QuerySpec) -> List[RawNewsRecord]:
        """
        RSS Feed íŒŒì‹±
        """
        feed = feedparser.parse(self.source.base_url)
        raw_records = []
        
        for entry in feed.entries:
            record = RawNewsRecord(
                source_id=self.source.id,
                source_name=self.source.name,
                raw_data=entry,  # feedparser entry dict
                raw_html=entry.get("summary", ""),
                extracted_text=strip_html(entry.get("summary", "")),
                url=entry["link"],
                fetch_timestamp=datetime.now()
            )
            raw_records.append(record)
        
        return raw_records

# Web í¬ë¡¤ëŸ¬
class WebCrawlerConnector(BaseConnector):
    async def fetch(self, query_spec: QuerySpec) -> List[RawNewsRecord]:
        """
        ì›¹ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (Selenium/Puppeteer)
        """
        async with AsyncChromium() as browser:
            page = await browser.new_page()
            await page.goto(self.source.base_url, timeout=30000)
            await page.wait_for_selector("article")  # ë‰´ìŠ¤ ì•„ì´í…œ ê¸°ë‹¤ë¦¬ê¸°
            
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            
            raw_records = []
            for article in soup.select("article"):
                record = RawNewsRecord(
                    source_id=self.source.id,
                    source_name=self.source.name,
                    raw_html=str(article),
                    extracted_text=article.get_text(strip=True),
                    url=article.select_one("a")["href"],
                    fetch_timestamp=datetime.now()
                )
                raw_records.append(record)
            
            return raw_records
```

---

### ëª¨ë“ˆ 4ï¸âƒ£: Parsing & Normalization (ì •ê·œí™”)

**ì—­í• **: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ê³µí†µ í•„ë“œë¡œ í‘œì¤€í™”

#### Normalized News Schema

```python
@dataclass
class NormalizedNews:
    # ì‹ë³„ì ë° ì›ë³¸ ì¶”ì 
    id: str                              # UUID
    raw_record_id: str                   # ì›ë³¸ RawNewsRecord ID
    source_id: str
    source_name: str
    source_tier: str                     # "whitelist" | "tier1" | "tier2" | "tier3"
    
    # í•µì‹¬ ì½˜í…ì¸ 
    title: str
    body: str                            # ì •ì œëœ ë³¸ë¬¸
    summary: Optional[str]               # ì§§ì€ ìš”ì•½
    
    # ë©”íƒ€ë°ì´í„°
    author: Optional[str]
    published_at: datetime               # ISO 8601, UTC ê¸°ì¤€
    updated_at: Optional[datetime]
    
    # ë¶„ë¥˜
    language: str                        # "ko", "en"
    country: str                         # "KR", "US"
    category: Optional[str]              # "ì •ì¹˜", "ê²½ì œ", "IT"
    section: Optional[str]               # ë” ì„¸ë¶„í™”ëœ ì„¹ì…˜
    tags: List[str]
    
    # ì¸ê¸°ë„ ë©”íƒ€
    view_count: Optional[int]
    share_count: Optional[int]
    comment_count: Optional[int]
    like_count: Optional[int]
    
    # ì°¸ì¡°
    url: str
    image_urls: List[str]
    
    # ë‚´ë¶€ ì²˜ë¦¬
    crawl_timestamp: datetime
    normalized_timestamp: datetime
```

#### Parser êµ¬í˜„

```python
class NewsParser:
    
    def parse(self, raw_record: RawNewsRecord, source: NewsSource) -> NormalizedNews:
        """
        ì†ŒìŠ¤ë³„ íŒŒì‹± ë¡œì§
        """
        parser_fn = self._get_parser_for_source(source.id)
        return parser_fn(raw_record)
    
    def _parse_naver_api(self, raw_record: RawNewsRecord) -> NormalizedNews:
        """
        Naver News API ì‘ë‹µ íŒŒì‹±
        """
        data = raw_record.raw_data
        
        # HTML ì •ì œ (ê´‘ê³  ì œê±°)
        body = self._clean_html(data.get("description", ""))
        
        # ì‹œê°„ íŒŒì‹± (Naver í˜•ì‹: "2026-02-05 10:30:00")
        pub_dt = self._parse_datetime(
            data.get("pubDate"),
            source_tz="Asia/Seoul"
        )
        
        return NormalizedNews(
            title=data["title"],
            body=body,
            author=data.get("author"),
            published_at=pub_dt,
            language="ko",
            country="KR",
            category=self._infer_category(data.get("category")),
            url=data["link"],
            source_id="naver_news",
            source_name="Naver News",
            source_tier="tier1"
        )
    
    def _parse_rss(self, raw_record: RawNewsRecord) -> NormalizedNews:
        """
        RSS í”¼ë“œ íŒŒì‹±
        """
        entry = raw_record.raw_data
        
        body = self._clean_html(entry.get("summary", ""))
        pub_dt = self._parse_datetime(entry.get("published"), "auto")
        
        return NormalizedNews(
            title=entry["title"],
            body=body,
            author=entry.get("author"),
            published_at=pub_dt,
            url=entry["link"],
            language=entry.get("language", "ko"),
            country="KR" if "ko" in entry.get("language", "") else "US"
        )
    
    def _clean_html(self, html_str: str) -> str:
        """
        HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        """
        soup = BeautifulSoup(html_str, "html.parser")
        
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
        for script in soup(["script", "style", "ad"]):
            script.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator="\n")
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        lines = [line.strip() for line in text.split("\n")]
        lines = [line for line in lines if line]
        
        return "\n".join(lines)
    
    def _parse_datetime(
        self, 
        date_str: str, 
        source_tz: str = "auto"
    ) -> datetime:
        """
        ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹± â†’ UTCë¡œ ì •ê·œí™”
        """
        # ì§€ì›í•˜ëŠ” í˜•ì‹ë“¤
        formats = [
            "%Y-%m-%d %H:%M:%S",        # "2026-02-05 10:30:00"
            "%Y-%m-%dT%H:%M:%SZ",       # ISO 8601
            "%a, %d %b %Y %H:%M:%S %z", # RSS
            "%Y/%m/%d %H:%M",
        ]
        
        dt = None
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                break
            except ValueError:
                continue
        
        if not dt:
            # Fallback: íŒŒì´ì¬ dateutil ì‚¬ìš©
            dt = dateutil.parser.parse(date_str)
        
        # í˜„ì§€ ì‹œê°„ â†’ UTC
        if source_tz != "auto" and dt.tzinfo is None:
            tz = pytz.timezone(source_tz)
            dt = tz.localize(dt)
        
        # UTCë¡œ ë³€í™˜
        if dt.tzinfo:
            dt = dt.astimezone(pytz.UTC)
        else:
            dt = dt.replace(tzinfo=pytz.UTC)
        
        return dt
    
    def _infer_category(self, category_hint: Optional[str]) -> Optional[str]:
        """
        ì†ŒìŠ¤ë³„ ì¹´í…Œê³ ë¦¬ â†’ í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
        """
        category_mapping = {
            "ì •ì¹˜": ["politics", "ì •ì¹˜", "êµ­íšŒ", "ëŒ€í†µë ¹"],
            "ê²½ì œ": ["economy", "ê²½ì œ", "ê¸°ì—…", "ì£¼ì‹"],
            "IT": ["tech", "it", "ê³¼í•™", "ê¸°ìˆ ", "ì†Œí”„íŠ¸ì›¨ì–´"],
            "ì‚¬íšŒ": ["society", "ì‚¬íšŒ", "ë²”ì£„", "êµìœ¡"],
        }
        
        if not category_hint:
            return None
        
        hint_lower = category_hint.lower()
        for standard_cat, keywords in category_mapping.items():
            if any(kw in hint_lower for kw in keywords):
                return standard_cat
        
        return None
```

---

### ëª¨ë“ˆ 5ï¸âƒ£: Dedup & Clustering (ì¤‘ë³µ ì œê±° ë° ì´ìŠˆ ë¬¶ê¸°)

**ì—­í• **: ë™ì¼í•œ ë‰´ìŠ¤ ì œê±°, ê°™ì€ ì´ìŠˆ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ê¸°

#### Deduplication Strategy

```python
class DeduplicationEngine:
    
    def deduplicate(self, news_list: List[NormalizedNews]) -> List[NormalizedNews]:
        """
        1ë‹¨ê³„: ì •í™•í•œ ì¤‘ë³µ ì œê±°
        2ë‹¨ê³„: ìœ ì‚¬ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§
        """
        
        # 1. URL ê¸°ë°˜ ì¤‘ë³µ (ê°€ì¥ ì—„ê²©)
        by_url = {}
        for news in news_list:
            normalized_url = self._normalize_url(news.url)
            if normalized_url not in by_url:
                by_url[normalized_url] = news
        
        # 2. ì œëª© í•´ì‹œ ê¸°ë°˜ (ì •í™•íˆ ê°™ì€ ì œëª©)
        by_title_hash = {}
        for news in by_url.values():
            title_hash = hashlib.md5(news.title.encode()).hexdigest()
            if title_hash not in by_title_hash:
                by_title_hash[title_hash] = news
        
        # 3. ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ì œëª© + ë³¸ë¬¸)
        clustered = self._cluster_similar(list(by_title_hash.values()))
        
        return clustered
    
    def _normalize_url(self, url: str) -> str:
        """
        URL ì •ê·œí™” (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°, ëŒ€ì†Œë¬¸ì í†µì¼)
        """
        parsed = urlparse(url)
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (utm ë“±)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        return normalized.lower()
    
    def _cluster_similar(self, news_list: List[NormalizedNews]) -> List[NormalizedNews]:
        """
        ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ê³ , ëŒ€í‘œ 1ê°œë§Œ ì„ ì •
        
        ìœ ì‚¬ë„ ê³„ì‚°: Jaccard ìœ ì‚¬ë„ (ë‹¨ì–´ ê¸°ë°˜)
        """
        if len(news_list) < 2:
            return news_list
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        def jaccard_similarity(title1: str, title2: str) -> float:
            """
            ë‘ ì œëª©ì˜ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„ (0~1)
            """
            words1 = set(title1.lower().split())
            words2 = set(title2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            return intersection / union
        
        # í´ëŸ¬ìŠ¤í„° ìƒì„± (ê·¸ë˜í”„ ê¸°ë°˜)
        clusters = []
        used = set()
        threshold = 0.6  # ìœ ì‚¬ë„ ì„ê³„ê°’
        
        for i, news in enumerate(news_list):
            if i in used:
                continue
            
            cluster = [news]
            used.add(i)
            
            # ìœ ì‚¬í•œ ë‰´ìŠ¤ ì°¾ê¸°
            for j in range(i + 1, len(news_list)):
                if j in used:
                    continue
                
                similarity = jaccard_similarity(
                    news_list[i].title,
                    news_list[j].title
                )
                
                if similarity >= threshold:
                    cluster.append(news_list[j])
                    used.add(j)
            
            clusters.append(cluster)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€í‘œ ë‰´ìŠ¤ ì„ ì •
        representative = []
        for cluster in clusters:
            # ì„ ì • ê¸°ì¤€: ê°€ì¥ ê¸´ ë³¸ë¬¸ (ìƒì„¸ ì •ë³´) + ê°€ì¥ ì˜¤ë˜ëœ (ì›ë³¸)
            best = max(
                cluster,
                key=lambda n: (len(n.body), -n.published_at.timestamp())
            )
            representative.append(best)
        
        return representative
```

#### Clustering Result (í´ëŸ¬ìŠ¤í„° ì €ì¥)

```python
@dataclass
class NewsCluster:
    """
    ê°™ì€ ì´ìŠˆì˜ ì—¬ëŸ¬ ë‰´ìŠ¤ ë¬¶ìŒ
    """
    cluster_id: str                      # UUID
    representative_news_id: str          # ëŒ€í‘œ ë‰´ìŠ¤ ID
    member_ids: List[str]                # í¬í•¨ëœ ë‰´ìŠ¤ IDë“¤
    issue_summary: str                   # ì´ìŠˆ ìš”ì•½
    member_count: int
    created_at: datetime
```

---

### ëª¨ë“ˆ 6ï¸âƒ£: Content Integrity QA (ì½˜í…ì¸  ë¬´ê²°ì„± ê²€ì¦) â­ í•µì‹¬

**ì—­í• **: ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ë° ì˜¤ì—¼ íƒì§€ (AI ë¯¸ì‚¬ìš©, ê·œì¹™ ê¸°ë°˜)

#### 6.1 Title-Body Consistency (ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„)

```python
class TitleBodyConsistencyChecker:
    
    def check(self, news: NormalizedNews) -> float:
        """
        ì œëª©ê³¼ ë³¸ë¬¸ì´ ì¼ì¹˜í•˜ëŠ” ì •ë„ ì ìˆ˜ (0~1)
        
        ì ìˆ˜ ê³„ì‚°:
        - ì œëª©ì˜ ì£¼ìš” ì—”í‹°í‹°ê°€ ë³¸ë¬¸ì—ë„ ë‚˜íƒ€ë‚˜ëŠ”ê°€?
        - ì œëª© í‚¤ì›Œë“œê°€ ë³¸ë¬¸ ìƒìœ„ N%ì— ëª°ë ¤ìˆëŠ”ê°€? (ë˜ëŠ” ë¶„ì‚°ë˜ì–´ ìˆëŠ”ê°€?)
        """
        
        # Step 1: ì œëª©ì—ì„œ ëª…ì‚¬/ì—”í‹°í‹° ì¶”ì¶œ
        title_entities = self._extract_entities(news.title)
        
        # Step 2: ë³¸ë¬¸ì—ì„œ ì—”í‹°í‹° ì¶œí˜„ í™•ì¸
        body_text = news.body.lower()
        entity_coverage = 0
        
        for entity in title_entities:
            if entity.lower() in body_text:
                entity_coverage += 1
        
        if not title_entities:
            return 1.0  # ì œëª© ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ í†µê³¼
        
        coverage_ratio = entity_coverage / len(title_entities)
        
        # Step 3: ì œëª© í‚¤ì›Œë“œ ë¶„ì‚°ë„ ê³„ì‚°
        title_words = set(news.title.lower().split())
        title_words = {w for w in title_words if len(w) > 2}  # ì§§ì€ ë‹¨ì–´ ì œì™¸
        
        # ë³¸ë¬¸ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        paragraphs = news.body.split("\n")
        word_distribution = []
        
        for para in paragraphs[:5]:  # ìƒìœ„ 5 ë¬¸ë‹¨ í™•ì¸
            para_lower = para.lower()
            count = sum(1 for w in title_words if w in para_lower)
            word_distribution.append(count)
        
        # ë¶„ì‚°ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ (ì œëª© ë‹¨ì–´ê°€ í•œ ë¬¸ë‹¨ì—ë§Œ ì§‘ì¤‘) ë¶ˆì¼ì¹˜
        max_concentration = max(word_distribution) if word_distribution else 0
        concentration_penalty = max_concentration / max(1, sum(word_distribution))
        
        # ìµœì¢… ì ìˆ˜
        consistency_score = coverage_ratio * (1 - concentration_penalty * 0.2)
        
        return min(1.0, max(0.0, consistency_score))
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        ì œëª©ì—ì„œ ëª…ì‚¬/ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        """
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (konlpy ë˜ëŠ” ê°„ë‹¨í•œ íŒ¨í„´)
        entities = []
        
        # íŒ¨í„´ 1: "íšŒì‚¬ëª…" (í•œê¸€ + ìˆ«ì/ì˜ë¬¸)
        import re
        company_pattern = r'[ê°€-í£]+(?:[0-9A-Za-z]+)?'
        matches = re.findall(company_pattern, text)
        entities.extend(matches)
        
        # íŒ¨í„´ 2: ì˜ë¬¸ ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        english_pattern = r'\b[A-Z][a-z]+\b'
        matches = re.findall(english_pattern, text)
        entities.extend(matches)
        
        return list(set(entities))
```

#### 6.2 Multi-topic Contamination (ì˜¤ì—¼ íƒì§€)

```python
class MultiTopicContaminationDetector:
    
    def check(self, news: NormalizedNews) -> tuple[float, List[str]]:
        """
        ë¬¸ë‹¨ë³„ í† í”½ì´ ë„ˆë¬´ ë‹¤ì–‘í•œê°€? (ì˜¤ì—¼ ê°€ëŠ¥ì„±)
        
        ë°˜í™˜ê°’:
        - contamination_score: 0~1 (1ì´ ë†’ì„ìˆ˜ë¡ ì˜¤ì—¼)
        - flags: ["unrelated_topics", "poor_coherence", ...]
        """
        
        flags = []
        
        # Step 1: ë³¸ë¬¸ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
        paragraphs = [p.strip() for p in news.body.split("\n") if p.strip()]
        
        if len(paragraphs) < 2:
            return 0.0, []  # ë‹¨ë½ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²€ì¦ ë¶ˆê°€
        
        # Step 2: ê° ë¬¸ë‹¨ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        paragraph_keywords = []
        for para in paragraphs[:10]:  # ìƒìœ„ 10ê°œ ë¬¸ë‹¨ë§Œ ë¶„ì„
            keywords = self._extract_keywords(para)
            paragraph_keywords.append(set(keywords))
        
        # Step 3: ë¬¸ë‹¨ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_scores = []
        for i in range(len(paragraph_keywords) - 1):
            sim = len(paragraph_keywords[i] & paragraph_keywords[i + 1]) / max(
                len(paragraph_keywords[i] | paragraph_keywords[i + 1]), 1
            )
            similarity_scores.append(sim)
        
        # Step 4: ìœ ì‚¬ë„ ë¶„ì„
        avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
        
        # ì—°ì†ëœ ë¬¸ë‹¨ì˜ ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì˜¤ì—¼ ê°€ëŠ¥ì„±
        low_similarity_count = sum(1 for s in similarity_scores if s < 0.2)
        contamination_score = 0.0
        
        if avg_similarity < 0.3:
            contamination_score = 0.7
            flags.append("unrelated_topics")
        elif low_similarity_count > len(similarity_scores) * 0.5:
            contamination_score = 0.5
            flags.append("inconsistent_topics")
        
        # Step 5: ë¶ˆê´€ë ¨ ë„ë©”ì¸ í‚¤ì›Œë“œ ê²€ì‚¬
        if self._has_unrelated_keywords(news.title, paragraph_keywords):
            contamination_score += 0.2
            flags.append("mixed_domains")
        
        return min(1.0, contamination_score), flags
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ë¬¸ë‹¨ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            "ì˜", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ê°™ì€", "ìˆë‹¤", "í•˜ë‹¤",
            "and", "the", "is", "at", "which"
        }
        
        words = text.lower().split()
        keywords = [w for w in words if len(w) > 2 and w not in stopwords]
        return keywords
    
    def _has_unrelated_keywords(self, title: str, paragraph_keywords: List[set]) -> bool:
        """
        ì œëª©ê³¼ ë¬´ê´€í•œ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— ë§ì€ê°€?
        """
        title_keywords = set(self._extract_keywords(title))
        
        # ì œëª©ê³¼ì˜ ê´€ê³„ì„± ë‚®ì€ ë¬¸ë‹¨ ì¹´ìš´íŠ¸
        low_relation_count = 0
        for para_keywords in paragraph_keywords:
            relation_ratio = len(title_keywords & para_keywords) / max(len(para_keywords), 1)
            if relation_ratio < 0.2:
                low_relation_count += 1
        
        # 50% ì´ìƒì˜ ë¬¸ë‹¨ì´ ì œëª©ê³¼ ë¬´ê´€í•˜ë©´ ì˜¤ì—¼ í”Œë˜ê·¸
        return low_relation_count > len(paragraph_keywords) * 0.5
```

#### 6.3 Boilerplate & Spam Detection (ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸/ìŠ¤íŒ¸ íƒì§€)

```python
class BoilerplateSpamDetector:
    
    def check(self, news: NormalizedNews) -> tuple[float, List[str]]:
        """
        ìŠ¤íŒ¸/ê´‘ê³ /ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ íƒì§€
        
        ë°˜í™˜ê°’:
        - spam_score: 0~1 (1ì´ ë†’ì„ìˆ˜ë¡ ìŠ¤íŒ¸)
        - flags: ["boilerplate", "ad_content", "gambling", "adult_content", ...]
        """
        
        flags = []
        spam_score = 0.0
        
        # 1. ë°˜ë³µ ë¬¸ì¥ íƒì§€
        if self._has_repetitive_sentences(news.body):
            spam_score += 0.3
            flags.append("repetitive_content")
        
        # 2. ê´‘ê³  ë¬¸êµ¬ íƒì§€
        if self._has_ad_keywords(news.body):
            spam_score += 0.3
            flags.append("ad_content")
        
        # 3. ë„ë°•/ì„±ì¸ ì½˜í…ì¸  íƒì§€
        if self._has_illegal_keywords(news.body):
            spam_score += 0.5
            flags.append("illegal_content")
        
        # 4. Lexical Density (ì˜ë¯¸ ë‹¨ì–´ ë¹„ìœ¨)
        lexical_density = self._calculate_lexical_density(news.body)
        if lexical_density < 0.4:  # ì˜ë¯¸ ë‹¨ì–´ ë¹„ìœ¨ì´ 40% ë¯¸ë§Œ
            spam_score += 0.2
            flags.append("low_content_quality")
        
        # 5. ë°”ì´ëŸ´/ë‚šì‹œ ì œëª© íŒ¨í„´
        if self._has_sensational_pattern(news.title):
            spam_score += 0.1
            flags.append("sensational_title")
        
        return min(1.0, spam_score), flags
    
    def _has_repetitive_sentences(self, text: str) -> bool:
        """
        ê°™ì€ ë¬¸ì¥ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚˜ëŠ”ê°€?
        """
        sentences = text.split(".")
        sentence_hashes = [hashlib.md5(s.strip().encode()).hexdigest() for s in sentences]
        
        # ì¤‘ë³µë„ ê³„ì‚°
        unique_count = len(set(sentence_hashes))
        total_count = len(sentence_hashes)
        
        if total_count < 3:
            return False
        
        repetition_ratio = 1 - (unique_count / total_count)
        return repetition_ratio > 0.3  # 30% ì´ìƒ ì¤‘ë³µ
    
    def _has_ad_keywords(self, text: str) -> bool:
        """
        ê´‘ê³ ì„± í‚¤ì›Œë“œ íƒì§€
        """
        ad_keywords = [
            "í´ë¦­", "ì§€ê¸ˆêµ¬ë§¤", "í• ì¸", "íŠ¹ê°€", "ë¬´ë£Œë°°ì†¡",
            "click here", "buy now", "limited offer", "free shipping",
            "ê´‘ê³ ", "sponsored", "promoted"
        ]
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in ad_keywords)
    
    def _has_illegal_keywords(self, text: str) -> bool:
        """
        ë„ë°•, ì„±ì¸, ë¶ˆë²• ì½˜í…ì¸  íƒì§€
        """
        illegal_keywords = [
            "ë„ë°•", "ì¹´ì§€ë…¸", "ì„±ì¸", "ì„¹ìŠ¤", "ìŒë€",
            "gambling", "casino", "adult", "porn",
            "ë¶ˆë²•", "ë§ˆì•½", "ì‚´ì¸", "ê°•ê°„"
        ]
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in illegal_keywords)
    
    def _calculate_lexical_density(self, text: str) -> float:
        """
        ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ì˜ ë¹„ìœ¨ (0~1)
        
        ì˜ë¯¸ ë‹¨ì–´: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬
        ë¶ˆì˜ë¯¸: ì¡°ì‚¬, ì¡°ë™ì‚¬ ë“±
        """
        words = text.lower().split()
        
        # ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ëª©ë¡ (í•œê¸€)
        function_words = {
            "ì˜", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ë¡œ", "ê³¼", "ê·¸ë¦¬ê³ ",
            "ë˜ëŠ”", "ìˆë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "ì•„ë‹ˆë‹¤"
        }
        
        meaningful_words = [w for w in words if w not in function_words and len(w) > 1]
        
        if not words:
            return 0.0
        
        return len(meaningful_words) / len(words)
    
    def _has_sensational_pattern(self, title: str) -> bool:
        """
        ë‚šì‹œ/ë°”ì´ëŸ´ ì œëª© íŒ¨í„´ íƒì§€
        """
        sensational_patterns = [
            r".*\[ì¶©ê²©\].*",
            r".*\[ê²½ì•…\].*",
            r".*\d+ë²ˆ\s(ì´ê²ƒ|ì €ê²ƒ).*",
            r".*ì´\sì‚¬ì‹¤ì¼\së¦¬\sì—†ë‹¤.*",
            r".*ë†€ë¼ìš´\s(ë°œí‘œ|ë¹„ë°€|ì§„ì‹¤).*",
            r".*\(í˜¹ì€.*\).*\(í˜¹ì€.*\).*",  # ê³¼ë„í•œ ê´„í˜¸
        ]
        
        for pattern in sensational_patterns:
            if re.search(pattern, title):
                return True
        
        return False
```

#### 6.4 Integrity Score ì¢…í•©

```python
class ContentIntegrityAssessment:
    
    def assess(self, news: NormalizedNews) -> tuple[float, dict]:
        """
        ìµœì¢… ë¬´ê²°ì„± ì ìˆ˜ ê³„ì‚°
        
        ë°˜í™˜ê°’:
        - integrity_score: 0~1 (1ì´ ë†’ì„ìˆ˜ë¡ ì–‘ì§ˆ)
        - details: ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        
        # ê° ê²€ì¦ ì‹¤í–‰
        title_body_consistency = TitleBodyConsistencyChecker().check(news)
        contamination_score, contamination_flags = MultiTopicContaminationDetector().check(news)
        spam_score, spam_flags = BoilerplateSpamDetector().check(news)
        
        # ì¢…í•© ì ìˆ˜ (ì—­ìˆ˜: spam/contaminationì€ ì ìˆ˜ë¥¼ ë‚®ì¶¤)
        integrity_score = (
            title_body_consistency * 0.4 +
            (1 - contamination_score) * 0.3 +
            (1 - spam_score) * 0.3
        )
        
        details = {
            "title_body_consistency": title_body_consistency,
            "contamination_score": contamination_score,
            "contamination_flags": contamination_flags,
            "spam_score": spam_score,
            "spam_flags": spam_flags,
            "final_integrity_score": integrity_score
        }
        
        return integrity_score, details
```

---

### ëª¨ë“ˆ 7ï¸âƒ£: Credibility & Quality Scoring (ì‹ ë¢°ë„/í’ˆì§ˆ ì ìˆ˜) â­ í•µì‹¬

### 1.1 ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­

#### A. ê²€ìƒ‰ ë° í•„í„°ë§
| ê¸°ëŠ¥ | ì„¤ëª… | ì…ë ¥ | ì¶œë ¥ |
|------|------|------|------|
| íŠ¹ì • ë‚ ì§œ ê²€ìƒ‰ | YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •í™•í•œ ë‚ ì§œ ê²€ìƒ‰ | ë‚ ì§œ | í•´ë‹¹ ë‚ ì§œ ë‰´ìŠ¤ |
| ê¸°ê°„ ê²€ìƒ‰ | ì‹œì‘ì¼~ì¢…ë£Œì¼ ë²”ìœ„ ë‚´ ë‰´ìŠ¤ | ì‹œì‘ì¼, ì¢…ë£Œì¼ | ë²”ìœ„ ë‚´ ë‰´ìŠ¤ |
| í‚¤ì›Œë“œ ê²€ìƒ‰ | ì œëª©, ë‚´ìš©, ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ | í‚¤ì›Œë“œ (ë‹¨ì¼/ë³µí•©) | ë§¤ì¹­ëœ ë‰´ìŠ¤ |
| ì¹´í…Œê³ ë¦¬ í•„í„° | ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, IT ë“± ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ | ì¹´í…Œê³ ë¦¬ëª… | í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ |

#### B. ìˆœìœ„ ë§¤ê¹€ ë° í’ˆì§ˆ í‰ê°€
| ì§€í‘œ | ê³„ì‚° ë°©ì‹ | ê°€ì¤‘ì¹˜ | ë²”ìœ„ |
|------|---------|--------|------|
| ì‹ ì„ ë„ | (í˜„ì¬ì‹œê°„ - ë°œí–‰ì‹œê°„) ì—­ìˆ˜ | ì„¤ì • ê°€ëŠ¥ | 0~100 |
| ì‹ ë¢°ë„ | ì¶œì²˜ ë“±ê¸‰ + ê²€ì¦ë„ | ì„¤ì • ê°€ëŠ¥ | 0~100 |
| ì¸ê¸°ë„ | ì¡°íšŒìˆ˜ + ê³µìœ ìˆ˜ + ëŒ“ê¸€ìˆ˜ ì •ê·œí™” | ì„¤ì • ê°€ëŠ¥ | 0~100 |
| ì œëª©-ë‚´ìš© ì¼ì¹˜ë„ | ì œëª© í‚¤ì›Œë“œê°€ ë‚´ìš©ì— í¬í•¨ëœ ë¹„ìœ¨ | ì„¤ì • ê°€ëŠ¥ | 0~100 |
| ì¤‘ë³µë„ | ìœ ì‚¬ ë‰´ìŠ¤ ìˆ˜ | ê°ì†Œ ê°€ì¤‘ì¹˜ | -10~0 |

#### C. ì¡°íšŒ ê¸°ëŠ¥
- íŠ¹ì • ë‚ ì§œì˜ ì¸ê¸° ë‰´ìŠ¤ Top N
- íŠ¹ì • ê¸°ê°„ì˜ ë§ì´ë³¸ ë‰´ìŠ¤ Top N
- ë³µí•© í•„í„° ì¡°í•© ê²€ìƒ‰ (ë‚ ì§œ + í‚¤ì›Œë“œ + ì¹´í…Œê³ ë¦¬)

### 1.2 ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­

| ìš”êµ¬ì‚¬í•­ | ëª©í‘œ | ë°©ë²• |
|---------|------|------|
| ì„±ëŠ¥ | 1000ê°œ ë‰´ìŠ¤ ì¡°íšŒ < 1ì´ˆ | ì¸ë±ì‹±, ìºì‹± |
| í™•ì¥ì„± | ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ê°€ ìš©ì´ | í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜ |
| ìœ ì§€ë³´ìˆ˜ì„± | ì½”ë“œ ìˆ˜ì • ìµœì†Œí™” | ì„¤ì • íŒŒì¼ ê¸°ë°˜ |
| ì‹ ë¢°ì„± | ë°ì´í„° ì†ì‹¤ ë°©ì§€ | íŠ¸ëœì­ì…˜, ë°±ì—… |

**ì—­í• **: ë‰´ìŠ¤ ì¶œì²˜ ì‹ ë¢°ë„, ì¦ê±°ë„, ì„ ì •ì„± ë“±ì„ ì¢…í•© ì ìˆ˜í™”

#### 7.1 Source Trust Score (ì¶œì²˜ ì‹ ë¢°ë„)

```python
class SourceTrustScorer:
    
    def score(self, source: NewsSource) -> float:
        """
        ì¶œì²˜ ì‹ ë¢°ë„ ì ìˆ˜ (0~1)
        
        ê¸°ì¤€:
        1. Tier ê¸°ë°˜ ê¸°ë³¸ ì ìˆ˜
        2. ë™ì  ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì„ íƒ)
        """
        
        # Tier ê¸°ë³¸ ì ìˆ˜
        tier_scores = {
            "whitelist": 0.98,
            "tier1": 0.90,
            "tier2": 0.75,
            "tier3": 0.50,
            "blacklist": 0.0
        }
        
        base_score = tier_scores.get(source.tier, 0.5)
        
        # ë™ì  ì¡°ì • (ì„ íƒì‚¬í•­: ìµœê·¼ í‰íŒ)
        # - ìµœê·¼ 3ê°œì›”ê°„ ì˜¤ë³´ìœ¨ì´ ë†’ìœ¼ë©´ ê°ì 
        # - ì •ì • ë‰´ìŠ¤ê°€ ë§ìœ¼ë©´ ê°ì 
        dynamic_penalty = self._calculate_dynamic_penalty(source)
        
        trust_score = max(0.0, base_score - dynamic_penalty)
        
        return trust_score
    
    def _calculate_dynamic_penalty(self, source: NewsSource) -> float:
        """
        ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ê°ì  (0~0.3)
        """
        # êµ¬í˜„ (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ)
        # - ìµœê·¼ ê¸°ì‚¬ ì¤‘ ì •ì •ëœ ë¹„ìœ¨
        # - ì‚¬ì‹¤ í™•ì¸ ì„œë¹„ìŠ¤ì—ì„œ "ê±°ì§“" íŒì • ë¹„ìœ¨
        return 0.0  # ì´ˆê¸°ê°’
```

#### 7.2 Evidence Score (ì¦ê±°ë„)

```python
class EvidenceScorer:
    
    def score(self, news: NormalizedNews) -> float:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì¦ê±° ì¶©ë¶„ì„± (0~1)
        
        ì ê²€ í•­ëª©:
        1. ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€
        2. ì§ì ‘ ì¸ìš© (ë”°ì˜´í‘œ) ì—¬ë¶€
        3. ê³µì‹ ë°œí‘œ ì–¸ê¸‰ ì—¬ë¶€
        4. ì¶œì²˜ ë§í¬ í¬í•¨ ì—¬ë¶€
        """
        
        evidence_points = 0
        max_points = 4
        
        # 1. ìˆ«ì/í†µê³„ í¬í•¨
        if self._has_statistics(news.body):
            evidence_points += 1
        
        # 2. ì§ì ‘ ì¸ìš©
        if self._has_direct_quotes(news.body):
            evidence_points += 1
        
        # 3. ê³µì‹ ë°œí‘œ/ì„±ëª…
        if self._has_official_statement(news.body):
            evidence_points += 1
        
        # 4. ì¶œì²˜ ë§í¬ (ë³¸ë¬¸ ë‚´ ë§í¬ ìˆ˜)
        link_count = self._count_reference_links(news.body)
        if link_count >= 1:
            evidence_points += 1
        
        evidence_score = evidence_points / max_points
        
        return evidence_score
    
    def _has_statistics(self, text: str) -> bool:
        """
        ìˆ«ì, ë°±ë¶„ìœ¨, í†µê³„ ë°ì´í„° í¬í•¨ ì—¬ë¶€
        """
        patterns = [
            r'\d+%',                    # ë°±ë¶„ìœ¨
            r'ì•½?\s*\d+[,\.]?\d*(?:ëª…|ê°œ|ê±´|ê±´)',  # ìˆ˜ëŸ‰
            r'\d+ë…„|\d+ì›”|\d+ì¼',          # ì‹œê°„
        ]
        
        return any(re.search(p, text) for p in patterns)
    
    def _has_direct_quotes(self, text: str) -> bool:
        """
        í°ë”°ì˜´í‘œë‚˜ ì‘ì€ë”°ì˜´í‘œ í¬í•¨ ì—¬ë¶€
        """
        return '"' in text or "'" in text or '"' in text or '"' in text
    
    def _has_official_statement(self, text: str) -> bool:
        """
        ê³µì‹ ë°œí‘œ/ì„±ëª…/ë³´ë„ìë£Œ ì–¸ê¸‰ ì—¬ë¶€
        """
        keywords = [
            "ë°œí‘œ", "ì„±ëª…", "ë³´ë„ìë£Œ", "ê³µì‹",
            "statement", "announcement", "official"
        ]
        
        text_lower = text.lower()
        return any(kw in text_lower for kw in keywords)
    
    def _count_reference_links(self, text: str) -> int:
        """
        ë³¸ë¬¸ ë‚´ ì°¸ì¡° ë§í¬ ê°œìˆ˜
        """
        return len(re.findall(r'https?://\S+', text))
```

#### 7.3 Sensationalism Penalty (ì„ ì •ì„± ê°ì )

```python
class SensationalismDetector:
    
    def calculate_penalty(self, news: NormalizedNews) -> float:
        """
        ì„ ì •ì„±/ê³¼ì¥ì„± ê°ì  (0~0.4)
        
        ê°ì  í•­ëª©:
        1. ê°ì • ë‹¨ì–´ (ì¶©ê²©, ê²½ì•…, ë¶„ë…¸ ë“±)
        2. ê³¼ì¥ëœ ë‹¨ì–´ (ìµœì´ˆ, ìµœê³ , ìµœì•… ë“±)
        3. í™•ì‹¤í•˜ì§€ ì•Šì€ í‘œí˜„ (~ì¸ ë“¯, ~í•  ê²ƒìœ¼ë¡œ ë³´ì„)
        4. ë„ë°œì  í‘œí˜„
        """
        
        penalty = 0.0
        title = news.title.lower()
        body = news.body.lower()
        
        # 1. ê°ì • ë‹¨ì–´
        emotion_words = {
            "ì¶©ê²©": 0.05,
            "ê²½ì•…": 0.05,
            "ë¶„ë…¸": 0.04,
            "ë…¼ë€": 0.03,
            "íŒŒì¥": 0.03,
            "í™”ì œ": 0.02
        }
        
        for word, score in emotion_words.items():
            if word in title or word in body[:500]:  # ì œëª©ê³¼ ë³¸ë¬¸ ì´ˆë°˜
                penalty += score
                break
        
        # 2. ê³¼ì¥ ë‹¨ì–´
        superlative_words = {
            "ìµœì´ˆ": 0.05,
            "ìµœê³ ": 0.04,
            "ìµœì•…": 0.05,
            "ìœ ì¼": 0.04,
            "ì—­ì‚¬ìƒ": 0.04
        }
        
        for word, score in superlative_words.items():
            if word in title:
                penalty += score
                break
        
        # 3. ë¶ˆí™•ì‹¤í•œ í‘œí˜„
        uncertain_patterns = [
            r'~\s*ì¸\s*ë“¯',
            r'~\s*í• \s*ê²ƒìœ¼ë¡œ\s*ë³´ì„',
            r'~\s*ì¼\s*ê°€ëŠ¥ì„±',
            r'~\s*ê²ƒ\s*ê°™ë‹¤'
        ]
        
        uncertain_count = sum(
            len(re.findall(p, body)) for p in uncertain_patterns
        )
        
        if uncertain_count > len(body.split()) * 0.1:  # ë‹¨ì–´ì˜ 10% ì´ìƒ
            penalty += 0.08
        
        # 4. ë„ë°œì  í‘œí˜„
        provocative_patterns = [
            r'\[ì¶©ê²©\]',
            r'\[ê²½ì•…\]',
            r'í´ë¦­\s*ìœ ë„',
            r'ë…¼ë€\s*ê°€ëŠ¥ì„±'
        ]
        
        if any(re.search(p, title) for p in provocative_patterns):
            penalty += 0.1
        
        return min(0.4, penalty)
```

#### 7.4 Cross-source Verification (í¬ë¡œìŠ¤ ê²€ì¦)

```python
class CrossSourceVerifier:
    
    async def calculate_verification_bonus(
        self, 
        news: NormalizedNews,
        all_news: List[NormalizedNews],
        source_registry: Dict[str, NewsSource]
    ) -> float:
        """
        ê°™ì€ ì´ìŠˆê°€ ë‹¤ë¥¸ ì‹ ë¢°ë„ ë†’ì€ ì†ŒìŠ¤ì—ì„œë„ ë³´ë„ë˜ì—ˆëŠ”ê°€?
        
        ë³´ë„ˆìŠ¤ ë¡œì§:
        - ê°™ì€ ë‚  ê°™ì€ ì´ìŠˆ ë‹¤ë¥¸ ì†ŒìŠ¤ 1ê°œ ì´ìƒ: +0.05
        - Tier1 ì´ìƒ ì†ŒìŠ¤ì—ì„œ ë³´ë„: +0.10
        - 3ê°œ ì´ìƒ ì†ŒìŠ¤ì—ì„œ ë³´ë„: +0.15
        """
        
        bonus = 0.0
        
        # ì´ìŠˆ ë™ì¼ì„± íŒë‹¨ (ê°„ë‹¨í•œ ë°©ë²•)
        # - ì œëª© ìœ ì‚¬ë„ >= 0.5
        # - ë°œí–‰ ì‹œê°„ 1ì¼ ì´ë‚´
        
        similar_news = self._find_similar_news(news, all_news)
        
        if not similar_news:
            return 0.0  # ë‹¤ë¥¸ ì†ŒìŠ¤ì—ì„œ ë³´ë„ ì•ˆ í•¨
        
        # Tierë³„ë¡œ ë¶„ë¥˜
        tier1_count = sum(
            1 for n in similar_news
            if source_registry.get(n.source_id, {}).get("tier") == "tier1"
        )
        
        # ë³´ë„ˆìŠ¤ ê³„ì‚°
        if len(similar_news) >= 1:
            bonus += 0.05
        if tier1_count >= 1:
            bonus += 0.05
        if len(similar_news) >= 3:
            bonus += 0.05
        
        return min(0.15, bonus)
    
    def _find_similar_news(
        self,
        news: NormalizedNews,
        all_news: List[NormalizedNews]
    ) -> List[NormalizedNews]:
        """
        ê°™ì€ ì´ìŠˆì˜ ë‹¤ë¥¸ ë‰´ìŠ¤ ì°¾ê¸°
        """
        similar = []
        
        for other in all_news:
            if other.id == news.id or other.source_id == news.source_id:
                continue
            
            # ì œëª© ìœ ì‚¬ë„
            similarity = self._jaccard_similarity(
                news.title.lower().split(),
                other.title.lower().split()
            )
            
            # ì‹œê°„ ì°¨ì´
            time_diff = abs(
                (news.published_at - other.published_at).total_seconds()
            )
            
            # ì¡°ê±´: ìœ ì‚¬ë„ >= 0.5 AND 1ì¼ ì´ë‚´
            if similarity >= 0.5 and time_diff <= 86400:
                similar.append(other)
        
        return similar
    
    def _jaccard_similarity(self, list1: List[str], list2: List[str]) -> float:
        """
        Jaccard ìœ ì‚¬ë„ (0~1)
        """
        set1 = set(list1)
        set2 = set(list2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        if union == 0:
            return 0.0
        
        return intersection / union
```

#### 7.5 Credibility & Quality ì¢…í•©

```python
class CredibilityQualityScorer:
    
    async def score(
        self,
        news: NormalizedNews,
        source: NewsSource,
        all_news: List[NormalizedNews],
        source_registry: Dict[str, NewsSource]
    ) -> tuple[float, float]:
        """
        ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ ë° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        
        ë°˜í™˜ê°’:
        - credibility_score: 0~1 (ì¶œì²˜ + í¬ë¡œìŠ¤ ê²€ì¦)
        - quality_score: 0~1 (ì¦ê±° + ì„ ì •ì„± - ìŠ¤íŒ¸)
        """
        
        # Credibility Score
        source_trust = SourceTrustScorer().score(source)
        verification_bonus = await CrossSourceVerifier().calculate_verification_bonus(
            news, all_news, source_registry
        )
        
        credibility_score = min(1.0, source_trust + verification_bonus)
        
        # Quality Score
        evidence = EvidenceScorer().score(news)
        sensationalism_penalty = SensationalismDetector().calculate_penalty(news)
        
        quality_score = max(0.0, evidence - sensationalism_penalty)
        
        return credibility_score, quality_score
```

---

### ëª¨ë“ˆ 8ï¸âƒ£: Popularity Engine (ì¸ê¸°ë„ ì ìˆ˜)

**ì—­í• **: ì¡°íšŒìˆ˜, ê³µìœ , ëŒ“ê¸€, íŠ¸ë Œë“œ ê¸°ë°˜ ì¸ê¸°ë„ ê³„ì‚°

```python
class PopularityScorer:
    
    def score(self, news: NormalizedNews, source: NewsSource) -> float:
        """
        ì¸ê¸°ë„ ì ìˆ˜ (0~1)
        
        ì ìˆ˜ êµ¬ì„±:
        - ì¡°íšŒìˆ˜ (40%)
        - ê³µìœ ìˆ˜ (35%)
        - ëŒ“ê¸€ìˆ˜ (25%)
        
        ì •ê·œí™”: ìƒìœ„ 5% ê¸°ì¤€
        """
        
        # ì—†ëŠ” ë©”íŠ¸ë¦­ì€ ëŒ€ì²´ ì§€í‘œ ì‚¬ìš©
        views = news.view_count or 0
        shares = news.share_count or 0
        comments = news.comment_count or 0
        
        # ì •ê·œí™” (ìƒìœ„ 5% ê°’ ê¸°ì¤€)
        # êµ¬í˜„: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœê·¼ ë‰´ìŠ¤ì˜ ìƒìœ„ 5% ê°’ ì¡°íšŒ
        norm_view = self._normalize(views, percentile_95_views=10000)
        norm_share = self._normalize(shares, percentile_95_shares=500)
        norm_comment = self._normalize(comments, percentile_95_comments=1000)
        
        popularity_score = (
            norm_view * 0.40 +
            norm_share * 0.35 +
            norm_comment * 0.25
        )
        
        return min(1.0, popularity_score)
    
    def _normalize(self, value: int, percentile_95_value: int) -> float:
        """
        0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        """
        if percentile_95_value == 0:
            return 0.0
        
        normalized = value / percentile_95_value
        return min(1.0, normalized)
```

---

### ëª¨ë“ˆ 9ï¸âƒ£: Ranker & Policy Filter (ìµœì¢… ì •ì±… í•„í„° ë° ë­í‚¹)

**ì—­í• **: ìµœì¢… ì ìˆ˜ ê³„ì‚°, ì •ì±… ì ìš©, ë‹¤ì–‘ì„± ë³´ì¥, ê²°ê³¼ ì •ë ¬

#### 9.1 ìµœì¢… ì ìˆ˜ ê³µì‹

```python
class FinalScoreCalculator:
    
    def calculate(
        self,
        news: NormalizedNews,
        integrity_score: float,
        credibility_score: float,
        quality_score: float,
        popularity_score: float,
        config: ScoringConfig
    ) -> float:
        """
        ìµœì¢… ì ìˆ˜ ê³„ì‚° ê³µì‹:
        
        FinalScore = (
            Popularity * w1 +
            Relevance * w2 +
            Quality * w3 +
            Credibility * w4 -
            SpamPenalty
        )
        
        Where:
        - Relevance = IntegrityScore (ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„)
        - Quality = (QualityScore + avg(Evidence, -Sensationalism))
        - SpamPenalty = 1 - IntegrityScore (ì˜¤ì—¼/ìŠ¤íŒ¸ì´ ë†’ìœ¼ë©´ ê°ì )
        """
        
        # Relevance = Content Integrity
        relevance = integrity_score
        
        # Quality = Quality + Evidence
        adjusted_quality = (quality_score + credibility_score) / 2
        
        # Spam Penalty
        spam_penalty = (1 - integrity_score) * 0.1  # ìµœëŒ€ ê°ì  0.1
        
        # ê°€ì¤‘ í‰ê· 
        weights = config.final_weights
        final_score = (
            popularity_score * weights["popularity"] +
            relevance * weights["relevance"] +
            adjusted_quality * weights["quality"] +
            credibility_score * weights["credibility"]
        ) - spam_penalty
        
        # 0~100 ë²”ìœ„ë¡œ ì •ê·œí™”
        final_score = max(0, min(100, final_score * 100))
        
        return final_score
```

#### 9.2 ì •ì±… í•„í„°

```python
class PolicyFilter:
    
    def apply(
        self,
        news_list: List[NewsWithScores],
        policy: FilterPolicy
    ) -> List[NewsWithScores]:
        """
        ì •ì±… ê¸°ë°˜ ë‰´ìŠ¤ í•„í„°ë§
        
        ì •ì±…:
        1. IntegrityScore ì„ê³„ê°’
        2. CredibilityScore ì„ê³„ê°’
        3. ì†ŒìŠ¤ ë‹¤ì–‘ì„±
        4. ì¤‘ë³µ ì œê±°
        """
        
        filtered = news_list.copy()
        
        # 1. Integrity í•„í„° (ì œê±°)
        filtered = [
            n for n in filtered
            if n.integrity_score >= policy.min_integrity_score  # ê¸°ë³¸: 0.5
        ]
        
        # 2. Credibility í•„í„° (ì˜ì‹¬ ë ˆì´ì–´ë¡œ ë¶„ë¦¬ ë˜ëŠ” ì œê±°)
        if policy.strict_mode:
            filtered = [
                n for n in filtered
                if n.credibility_score >= policy.min_credibility_score  # ê¸°ë³¸: 0.7
            ]
        else:
            # ì˜ì‹¬ ë‰´ìŠ¤ ë¶„ë¦¬ (ë‚®ì€ ì‹ ë¢°ë„)
            trusted = [
                n for n in filtered
                if n.credibility_score >= policy.min_credibility_score
            ]
            suspicious = [
                n for n in filtered
                if n.credibility_score < policy.min_credibility_score
            ]
            # ë‘˜ ë‹¤ ë°˜í™˜ (ì‚¬ìš©ìê°€ ì„ íƒ ê°€ëŠ¥)
            return trusted, suspicious
        
        # 3. ì†ŒìŠ¤ ë‹¤ì–‘ì„± (Top N ë‚´ ë™ì¼ media ìµœëŒ€ Kê°œ)
        filtered = self._apply_source_diversity(
            filtered,
            max_same_source=policy.max_same_source_in_top_n  # ê¸°ë³¸: 3
        )
        
        return filtered
    
    def _apply_source_diversity(
        self,
        news_list: List[NewsWithScores],
        max_same_source: int
    ) -> List[NewsWithScores]:
        """
        ê°™ì€ ì†ŒìŠ¤ì˜ ë‰´ìŠ¤ê°€ Top Nì—ì„œ Kê°œë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¡°ì •
        """
        result = []
        source_counts = {}
        
        for news in news_list:
            source = news.source_id
            
            if source_counts.get(source, 0) < max_same_source:
                result.append(news)
                source_counts[source] = source_counts.get(source, 0) + 1
        
        return result
```

#### 9.3 ê²°ê³¼ ì •ë ¬ ë° ê·¸ë£¹í•‘

```python
class ResultFormatter:
    
    def format(
        self,
        news_list: List[NewsWithScores],
        query_spec: QuerySpec
    ) -> Dict:
        """
        ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…
        """
        
        # ì •ë ¬
        sorted_news = self._sort(news_list, query_spec)
        
        # ê·¸ë£¹í•‘ (í•„ìš”ì‹œ)
        if query_spec.group_by == "day":
            grouped = self._group_by_day(sorted_news)
            return self._format_grouped(grouped, query_spec)
        else:
            return self._format_list(sorted_news, query_spec)
    
    def _sort(
        self,
        news_list: List[NewsWithScores],
        query_spec: QuerySpec
    ) -> List[NewsWithScores]:
        """
        ì •ë ¬ ê¸°ì¤€ ì ìš©
        """
        sort_key = query_spec.popularity_type
        
        if sort_key == "trending":
            # íŠ¸ë Œë“œ: ì‹œê°„ë‹¹ ìƒìŠ¹ë¥ 
            return sorted(
                news_list,
                key=lambda n: n.trending_velocity,
                reverse=True
            )
        elif sort_key == "popular":
            # ì¸ê¸°: ì¡°íšŒìˆ˜ ê¸°ë°˜
            return sorted(
                news_list,
                key=lambda n: n.popularity_score,
                reverse=True
            )
        elif sort_key == "latest":
            # ìµœì‹ : ë°œí–‰ ì‹œê°„
            return sorted(
                news_list,
                key=lambda n: n.published_at,
                reverse=True
            )
        else:  # quality (ê¸°ë³¸)
            # í’ˆì§ˆ: ìµœì¢… ì ìˆ˜
            return sorted(
                news_list,
                key=lambda n: n.final_score,
                reverse=True
            )
    
    def _group_by_day(
        self,
        news_list: List[NewsWithScores]
    ) -> Dict[str, List[NewsWithScores]]:
        """
        ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘
        """
        grouped = {}
        
        for news in news_list:
            day = news.published_at.date().isoformat()
            
            if day not in grouped:
                grouped[day] = []
            
            grouped[day].append(news)
        
        return grouped
    
    def _format_grouped(
        self,
        grouped: Dict[str, List[NewsWithScores]],
        query_spec: QuerySpec
    ) -> Dict:
        """
        ê·¸ë£¹í™”ëœ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        """
        result = {
            "status": "success",
            "query": {
                "date_from": query_spec.date_from.isoformat(),
                "date_to": query_spec.date_to.isoformat(),
                "categories": query_spec.category,
                "keywords": query_spec.keywords
            },
            "by_date": {}
        }
        
        for day, news_items in sorted(grouped.items(), reverse=True):
            result["by_date"][day] = {
                "count": len(news_items),
                "top_stories": [
                    self._serialize_news(n, include_scores=True)
                    for n in news_items[:5]  # ìƒìœ„ 5ê°œ
                ]
            }
        
        return result
    
    def _format_list(
        self,
        news_list: List[NewsWithScores],
        query_spec: QuerySpec
    ) -> Dict:
        """
        ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        """
        # í˜ì´ì§• ì ìš©
        start = query_spec.offset
        end = start + query_spec.limit
        paged_news = news_list[start:end]
        
        return {
            "status": "success",
            "query": {
                "date_from": query_spec.date_from.isoformat() if query_spec.date_from else None,
                "date_to": query_spec.date_to.isoformat() if query_spec.date_to else None,
                "categories": query_spec.category,
                "keywords": query_spec.keywords
            },
            "pagination": {
                "total": len(news_list),
                "limit": query_spec.limit,
                "offset": query_spec.offset
            },
            "news": [
                self._serialize_news(n, include_scores=True)
                for n in paged_news
            ]
        }
    
    def _serialize_news(
        self,
        news: NewsWithScores,
        include_scores: bool = True
    ) -> Dict:
        """
        ë‰´ìŠ¤ ê°ì²´ë¥¼ JSON ì§ë ¬í™”
        """
        result = {
            "id": news.id,
            "title": news.title,
            "summary": news.summary or news.body[:200],
            "source": {
                "id": news.source_id,
                "name": news.source_name,
                "tier": news.source_tier
            },
            "published_at": news.published_at.isoformat(),
            "category": news.category,
            "url": news.url,
            "image": news.image_urls[0] if news.image_urls else None
        }
        
        if include_scores:
            result["scores"] = {
                "final": round(news.final_score, 2),
                "integrity": round(news.integrity_score, 3),
                "credibility": round(news.credibility_score, 3),
                "quality": round(news.quality_score, 3),
                "popularity": round(news.popularity_score, 3)
            }
        
        return result
```

---

## ë°ì´í„° ëª¨ë¸ ì„¤ê³„

### Core News Entity (í†µí•©)

```python
@dataclass
class NewsWithScores:
    """
    ëª¨ë“  ì²˜ë¦¬ë¥¼ ê±°ì¹œ ìµœì¢… ë‰´ìŠ¤ ê°ì²´
    """
    # ê¸°ë³¸ ì •ë³´
    id: str
    raw_record_id: str
    source_id: str
    source_name: str
    source_tier: str
    
    # ì½˜í…ì¸ 
    title: str
    body: str
    summary: Optional[str]
    author: Optional[str]
    published_at: datetime
    updated_at: Optional[datetime]
    
    # ë¶„ë¥˜
    language: str
    country: str
    category: Optional[str]
    tags: List[str]
    
    # ì¸ê¸°ë„
    view_count: Optional[int]
    share_count: Optional[int]
    comment_count: Optional[int]
    like_count: Optional[int]
    
    # ì°¸ì¡°
    url: str
    image_urls: List[str]
    
    # ì ìˆ˜ë“¤ (ëª¨ë“ˆë³„)
    integrity_score: float          # Module 6
    contamination_score: float
    spam_score: float
    credibility_score: float        # Module 7
    quality_score: float
    evidence_score: float
    sensationalism_penalty: float
    popularity_score: float         # Module 8
    trending_velocity: float        # ì¶”ê°€: ì‹œê°„ë‹¹ ìƒìŠ¹ë¥ 
    
    # ìµœì¢… ì ìˆ˜
    final_score: float              # 0~100
    
    # ì²˜ë¦¬ ë©”íƒ€
    crawl_timestamp: datetime
    processed_timestamp: datetime
    cluster_id: Optional[str]       # í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ë©´ í• ë‹¹
```

---

## ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹ ê³µì‹

### ì„¤ì • (config.yaml)

```yaml
# ìµœì¢… ì ìˆ˜ ê°€ì¤‘ì¹˜
scoring:
  final_weights:
    popularity: 0.25               # ì¸ê¸°ë„
    relevance: 0.25                # ê´€ë ¨ì„± (ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„)
    quality: 0.25                  # í’ˆì§ˆ (ì¦ê±°ë„)
    credibility: 0.25              # ì‹ ë¢°ë„
  
  # í†µí•© ì •ì±…
  integrity_threshold: 0.5         # IntegrityScore < 0.5ë©´ ì œì™¸
  credibility_threshold: 0.6       # CredibilityScore < 0.6ë©´ "ì˜ì‹¬" ë¶„ë¥˜
  
  source_diversity:
    max_same_source_in_top_n: 3    # Top 20ì— ë™ì¼ ì†ŒìŠ¤ ìµœëŒ€ 3ê°œ
    enforce: true

# Ranking Presets
ranking_presets:
  quality:
    name: "í’ˆì§ˆìˆœ"
    weights:
      popularity: 0.15
      relevance: 0.30
      quality: 0.40
      credibility: 0.15
  
  trending:
    name: "íŠ¸ë Œë”©"
    weights:
      popularity: 0.50
      relevance: 0.10
      quality: 0.20
      credibility: 0.20
  
  credible:
    name: "ì‹ ë¢°ë„ìˆœ"
    weights:
      popularity: 0.10
      relevance: 0.20
      quality: 0.20
      credibility: 0.50
  
  latest:
    name: "ìµœì‹ ìˆœ"
    weights:
      popularity: 0.05
      relevance: 0.10
      quality: 0.15
      credibility: 0.20
    sort_by: "published_at"  # ì‹œê°„ìˆœ ìš°ì„ 
```

---

## API ë° ì¸í„°í˜ì´ìŠ¤

### REST API

```
# ê¸°ë³¸ ê²€ìƒ‰
GET /api/v1/news/search?keyword=AI&category=IT&limit=20

# íŠ¹ì • ë‚ ì§œ ì¸ê¸° ë‰´ìŠ¤
GET /api/v1/news/trending?date=2026-02-05&limit=10

# ê¸°ê°„ë³„ ë§ì´ ë³¸ ë‰´ìŠ¤
GET /api/v1/news/popular?from_date=2026-02-01&to_date=2026-02-05&limit=20

# ê³ ê¸‰ ê²€ìƒ‰
POST /api/v1/news/search
{
  "query": {
    "keywords": ["AI", "ê¸°ìˆ "],
    "categories": ["IT"],
    "date_from": "2026-02-01",
    "date_to": "2026-02-05"
  },
  "filters": {
    "min_integrity": 0.5,
    "min_credibility": 0.6,
    "min_quality": 0.5
  },
  "ranking": "quality",
  "group_by": "day",
  "limit": 50
}
```

---

## êµ¬í˜„ ì „ëµ ë° ìˆœì„œ

### Phase êµ¬ì¡° (14ì£¼)

| Phase | ì£¼ì°¨ | ëª©í‘œ | í•µì‹¬ ì‚°ì¶œë¬¼ |
|-------|------|------|-----------|
| 1 | 1-2 | ê¸°ì´ˆ ì¸í”„ë¼ | DB, ì„¤ì •, ë°ì´í„° ëª¨ë¸ |
| 2 | 3-4 | ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ | Ingestion, Parsing, Validation |
| 3 | 5-6 | ì¤‘ë³µ ì œê±°/í´ëŸ¬ìŠ¤í„°ë§ | Dedup Engine, Clustering |
| 4 | 7-8 | ë¬´ê²°ì„± ê²€ì¦ | Content QA (Module 6) |
| 5 | 9-10 | ì‹ ë¢°ë„/í’ˆì§ˆ ì ìˆ˜ | Credibility Scorer (Module 7) |
| 6 | 11-12 | ì¸ê¸°ë„ + ìµœì¢… ë­í‚¹ | Popularity + Ranker (Module 8, 9) |
| 7 | 13-14 | API + CLI + í…ŒìŠ¤íŠ¸ | ì¸í„°í˜ì´ìŠ¤, í…ŒìŠ¤íŠ¸, ë°°í¬ |

---

## ì„±ëŠ¥ ë° í™•ì¥ì„±

### ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ë°©ë²• |
|--------|------|------|
| ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ | < 1ì´ˆ | ì¸ë±ì‹±, ìºì‹± |
| ë³‘ë ¬ ìˆ˜ì§‘ | 1000+ ê¸°ì‚¬/ë¶„ | async, ë©€í‹° ìŠ¤ë ˆë”© |
| ë©”ëª¨ë¦¬ | < 1GB (ê¸°ë³¸) | ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜ ì²˜ë¦¬ |
| ì •í™•ë„ | í’ˆì§ˆ ì ìˆ˜ ê²€ì¦ | ì •ê¸°ì  í‰ê°€ |

---

## ë§ˆì¹˜ë©°

ì´ ê¸°íšì„œëŠ” **ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ë‹¤ì¤‘ ì†ŒìŠ¤ì˜ ê³ í’ˆì§ˆ ë‰´ìŠ¤ë¥¼ ì •í™•í•˜ê²Œ ì œê³µ**í•˜ëŠ” ì‹œìŠ¤í…œì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

**í•µì‹¬ íŠ¹ì§•**:
âœ… 9ê°œ ëª¨ë“ˆì˜ ëª…í™•í•œ ë¶„ë¦¬  
âœ… ê·œì¹™ ê¸°ë°˜ ë¬´ê²°ì„±/ì‹ ë¢°ë„ ê²€ì¦ (AI ìµœì†Œí™”)  
âœ… ì„¤ì • ê¸°ë°˜ ìš´ì˜ (í•˜ë“œì½”ë”© ì œê±°)  
âœ… ë‹¤ì–‘í•œ ì •ë ¬ ë° í•„í„° ì˜µì…˜  
âœ… ê³ í™•ì¥ì„± ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (CLI/Web UI)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    API Layer / Router                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Search     â”‚  â”‚   Filter     â”‚  â”‚   Ranking    â”‚       â”‚
â”‚  â”‚   Engine     â”‚  â”‚   Engine     â”‚  â”‚   Engine     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   News       â”‚  â”‚   News       â”‚  â”‚   News       â”‚       â”‚
â”‚  â”‚   Crawler    â”‚  â”‚   Parser     â”‚  â”‚   Validator  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Database   â”‚  â”‚   Cache      â”‚  â”‚   Config     â”‚       â”‚
â”‚  â”‚   (ë‰´ìŠ¤ ì €ì¥)â”‚  â”‚   (ì¸ë±ìŠ¤)   â”‚  â”‚   (ì„¤ì •)     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   External   â”‚  â”‚   External   â”‚  â”‚   External   â”‚       â”‚
â”‚  â”‚   News API   â”‚  â”‚   RSS Feed   â”‚  â”‚   Web Crawl  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 í•µì‹¬ ëª¨ë“ˆ ì„¤ëª…

| ëª¨ë“ˆ | ì—­í•  | ì…ì¶œë ¥ |
|------|------|--------|
| **News Crawler** | ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ | ì¶œë ¥: ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„° |
| **News Parser** | HTML/JSON íŒŒì‹±, êµ¬ì¡°í™” | ì…ë ¥: ì›ë³¸ ë°ì´í„°, ì¶œë ¥: ì •ê·œí™”ëœ ë‰´ìŠ¤ |
| **News Validator** | ë°ì´í„° ìœ íš¨ì„± ê²€ì¦, ì¤‘ë³µ ì œê±° | ì…ë ¥: íŒŒì‹±ëœ ë‰´ìŠ¤, ì¶œë ¥: ê²€ì¦ëœ ë‰´ìŠ¤ |
| **Search Engine** | ì¸ë±ì‹±, ë¹ ë¥¸ ê²€ìƒ‰ | ì…ë ¥: ê²€ìƒ‰ ì¿¼ë¦¬, ì¶œë ¥: ë§¤ì¹­ëœ ë‰´ìŠ¤ ID ëª©ë¡ |
| **Filter Engine** | ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ | ì…ë ¥: ë‰´ìŠ¤ + í•„í„° ê·œì¹™, ì¶œë ¥: í•„í„°ë§ëœ ë‰´ìŠ¤ |
| **Ranking Engine** | ì ìˆ˜ ê³„ì‚°, ìˆœìœ„ ë§¤ê¹€ | ì…ë ¥: ë‰´ìŠ¤ + ê°€ì¤‘ì¹˜, ì¶œë ¥: ìˆœìœ„ ë§¤ê²¨ì§„ ë‰´ìŠ¤ |
| **Config Manager** | ì„¤ì • íŒŒì¼ ê´€ë¦¬ | ì…ë ¥: YAML/JSON, ì¶œë ¥: ì„¤ì • ê°ì²´ |
| **Cache Manager** | ì¸ë±ìŠ¤ ë° ì¿¼ë¦¬ ìºì‹± | ì…ë ¥: ë‰´ìŠ¤ ë°ì´í„°, ì¶œë ¥: ë¹ ë¥¸ ê²€ìƒ‰ |
| **Database** | ë‰´ìŠ¤ ì˜ì†ì„± ì €ì¥ | ì…ë ¥: ê²€ì¦ëœ ë‰´ìŠ¤, ì¶œë ¥: ì €ì¥ëœ ë‰´ìŠ¤ |

---

## 3ë‹¨ê³„: ë°ì´í„° ëª¨ë¸ ì„¤ê³„

### 3.1 í•µì‹¬ ì—”í‹°í‹°

#### News Entity (ë‰´ìŠ¤ ê°œì²´)
```
{
  "id": "unique_hash",                    // ë‰´ìŠ¤ ê³ ìœ  ID
  "title": "string",                      // ì œëª©
  "content": "string",                    // ë³¸ë¬¸ ë‚´ìš©
  "summary": "string",                    // ìš”ì•½ (ì„ íƒ)
  "source": "string",                     // ì¶œì²˜ëª… (ì˜ˆ: "ì¡°ì„ ì¼ë³´")
  "source_url": "string",                 // ì¶œì²˜ URL
  "url": "string",                        // ë‰´ìŠ¤ ì›ë³¸ URL
  "category": "string",                   // ì¹´í…Œê³ ë¦¬
  "published_at": "ISO 8601",             // ë°œí–‰ ì‹œê°„
  "crawled_at": "ISO 8601",               // ìˆ˜ì§‘ ì‹œê°„
  "metadata": {
    "views": integer,                     // ì¡°íšŒìˆ˜
    "shares": integer,                    // ê³µìœ ìˆ˜
    "comments": integer,                  // ëŒ“ê¸€ìˆ˜
    "author": "string",                   // ì €ì
    "tags": ["string"]                    // íƒœê·¸ë“¤
  },
  "quality_score": float,                 // ìµœì¢… í’ˆì§ˆ ì ìˆ˜ (0~100)
  "scores": {
    "freshness": float,                   // ì‹ ì„ ë„ ì ìˆ˜
    "credibility": float,                 // ì‹ ë¢°ë„ ì ìˆ˜
    "popularity": float,                  // ì¸ê¸°ë„ ì ìˆ˜
    "title_content_match": float           // ì œëª©-ë‚´ìš© ì¼ì¹˜ë„
  }
}
```

#### Source Entity (ë‰´ìŠ¤ ì¶œì²˜)
```
{
  "id": "string",
  "name": "string",                       // ì¶œì²˜ëª… (ì˜ˆ: "ì¡°ì„ ì¼ë³´")
  "type": "enum",                         // API, RSS, WEB_CRAWL
  "url": "string",
  "credibility_score": float,             // ì‹ ë¢°ë„ (0~100)
  "is_active": boolean,
  "update_interval": integer,             // ì—…ë°ì´íŠ¸ ê°„ê²© (ë¶„)
  "last_updated": "ISO 8601",
  "config": {                             // ìˆ˜ì§‘ ì„¤ì •
    "headers": {},
    "timeout": integer,
    "retry_count": integer
  }
}
```

#### Category Entity (ì¹´í…Œê³ ë¦¬)
```
{
  "id": "string",
  "name": "string",                       // ì˜ˆ: "ì •ì¹˜", "ê²½ì œ", "IT"
  "keywords": ["string"],                 // ë¶„ë¥˜ìš© í‚¤ì›Œë“œ
  "weight": float,                        // ìƒëŒ€ì  ê°€ì¤‘ì¹˜
  "is_active": boolean
}
```

### 3.2 ì„¤ì • ëª¨ë¸ (config.yaml)

```yaml
# í•„í„° ì„¤ì •
filters:
  min_content_length: 300              # ìµœì†Œ ë‚´ìš© ê¸¸ì´
  max_content_length: 50000            # ìµœëŒ€ ë‚´ìš© ê¸¸ì´
  exclude_keywords: []                 # ì œì™¸ í‚¤ì›Œë“œ
  require_keywords: []                 # í•„ìˆ˜ í‚¤ì›Œë“œ
  date_range_days: 30                  # ê¸°ë³¸ ê²€ìƒ‰ ë²”ìœ„ (ì¼)

# ì ìˆ˜ ê°€ì¤‘ì¹˜ ì„¤ì •
scoring:
  weights:
    freshness: 0.25                    # ì‹ ì„ ë„
    credibility: 0.30                  # ì‹ ë¢°ë„
    popularity: 0.25                   # ì¸ê¸°ë„
    title_content_match: 0.20           # ì œëª©-ë‚´ìš© ì¼ì¹˜ë„
  
  freshness:
    half_life_hours: 24                # ì‹ ì„ ë„ ë°˜ê° ì‹œê°„
  
  credibility:
    source_score_map:                  # ì¶œì²˜ë³„ ì‹ ë¢°ë„
      "ì¡°ì„ ì¼ë³´": 85
      "ì¤‘ì•™ì¼ë³´": 85
      "ë™ì•„ì¼ë³´": 85
      "ê²½í–¥ì‹ ë¬¸": 80
  
  popularity:
    view_weight: 0.4
    share_weight: 0.35
    comment_weight: 0.25
    normalization_factor: 10000        # ì •ê·œí™” ê³„ìˆ˜

# ìºì‹œ ì„¤ì •
cache:
  enabled: true
  ttl_minutes: 60
  max_size: 10000

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
database:
  type: "sqlite"                       # ë˜ëŠ” postgresql, mysql
  path: "./news.db"
  backup_enabled: true
  backup_interval_hours: 6

# ë‰´ìŠ¤ ì†ŒìŠ¤ ì„¤ì •
sources:
  - id: "naver_news"
    name: "Naver News"
    type: "api"
    url: "https://api.naver.com/..."
    is_active: true
    credibility_score: 88
    update_interval: 30
  
  - id: "google_news"
    name: "Google News"
    type: "rss"
    url: "https://feeds.google.com/..."
    is_active: true
    credibility_score: 85
    update_interval: 15

# ì¹´í…Œê³ ë¦¬ ì„¤ì •
categories:
  - id: "politics"
    name: "ì •ì¹˜"
    keywords: ["ëŒ€í†µë ¹", "ì˜íšŒ", "ì •ì±…", "ì„ ê±°"]
    weight: 1.0
    is_active: true
  
  - id: "economy"
    name: "ê²½ì œ"
    keywords: ["ì£¼ì‹", "ê¸ˆìœµ", "ê²½ì œ", "ê¸°ì—…"]
    weight: 1.0
    is_active: true
  
  - id: "tech"
    name: "IT/ê³¼í•™"
    keywords: ["ê¸°ìˆ ", "ì†Œí”„íŠ¸ì›¨ì–´", "AI", "ê°œë°œ"]
    weight: 1.0
    is_active: true
```

---

## 4ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

### 4.1 ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ íë¦„

```
ì‹œì‘
  â†“
í™œì„± ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ
  â†“
â”€â”€â”€ ê° ì†ŒìŠ¤ë³„ ë³‘ë ¬ ì²˜ë¦¬ â”€â”€â”€
â”‚                      â”‚
â†“ (API)              â†“ (RSS)              â†“ (Web Crawl)
API ìš”ì²­          RSS í”¼ë“œ íŒŒì‹±          HTML ìŠ¤í¬ë˜í•‘
â”‚                    â”‚                     â”‚
â†“                    â†“                     â†“
ë°ì´í„° ì¶”ì¶œ        í•­ëª© íŒŒì‹±              í˜ì´ì§€ íŒŒì‹±
â”‚                    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
      ì›ë³¸ ë°ì´í„°
          â†“
   ë°ì´í„° ì •ê·œí™” (Parser)
   - í•„ë“œ ë§¤í•‘
   - í…ìŠ¤íŠ¸ í´ë¦¬ë‹
   - ì‹œê°„ í¬ë§·íŒ…
          â†“
    ë°ì´í„° ê²€ì¦ (Validator)
    - í•„ìˆ˜ í•„ë“œ í™•ì¸
    - ì¤‘ë³µ ì œê±°
    - ê¸¸ì´ í™•ì¸
          â†“
     ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
          â†“
    ì¸ë±ìŠ¤ ê°±ì‹ 
          â†“
ì™„ë£Œ (ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸°)
```

### 4.2 ê° ì†ŒìŠ¤ íƒ€ì…ë³„ ìˆ˜ì§‘ ë°©ì‹

#### A. API ë°©ì‹ (ì˜ˆ: Naver News API)
```
ìš”ì²­:
  - URL: {API_BASE_URL}/news/list
  - Method: GET
  - íŒŒë¼ë¯¸í„°: category, limit, offset, date_from, date_to
  - Headers: Authorization, User-Agent

ì‘ë‹µ:
  {
    "status": "success",
    "data": [
      {
        "id": "...",
        "title": "...",
        "content": "...",
        "published": "2026-02-05T10:00:00Z",
        "category": "ê²½ì œ",
        "source": "ë„¤ì´ë²„ ë‰´ìŠ¤",
        "url": "...",
        "metadata": {
          "views": 1000,
          "shares": 50
        }
      }
    ]
  }

ì¥ì :
  - êµ¬ì¡°í™”ëœ ë°ì´í„°
  - ë¹ ë¥¸ ì‘ë‹µ
  - ë©”íƒ€ë°ì´í„° í’ë¶€
  
ë‹¨ì :
  - API ìš”ê¸ˆ ê°€ëŠ¥ì„±
  - Rate Limiting
  - ì„ íƒì  í•„í„°ë§
```

#### B. RSS Feed ë°©ì‹
```
ìš”ì²­:
  - URL: {RSS_FEED_URL}
  - Method: GET

ì‘ë‹µ (XML):
  <?xml version="1.0"?>
  <rss version="2.0">
    <channel>
      <item>
        <title>...</title>
        <link>...</link>
        <description>...</description>
        <pubDate>...</pubDate>
        <category>...</category>
      </item>
    </channel>
  </rss>

ì¥ì :
  - í‘œì¤€í™”ëœ í˜•ì‹
  - ë¬´ë£Œ
  - ëŒ€ë¶€ë¶„ì˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì§€ì›
  
ë‹¨ì :
  - ìš”ì•½ë§Œ ì œê³µ (ì „ë¬¸ í•„ìš” ì‹œ ì¬ìˆ˜ì§‘)
  - ì¡°íšŒìˆ˜/ê³µìœ ìˆ˜ ì—†ìŒ
```

#### C. Web Crawling ë°©ì‹
```
1. íƒ€ê²Ÿ í˜ì´ì§€ ì‹ë³„
   - ë‰´ìŠ¤ ëª©ë¡ URL
   - í˜ì´ì§€ êµ¬ì¡° ë¶„ì„

2. CSS/XPath ì„ íƒì ì‘ì„±
   - ì œëª©: div.title > h2
   - ë‚´ìš©: div.article-body
   - ë°œí–‰ì‹œê°„: span.date

3. ë™ì  ë¡œë”© ì²˜ë¦¬
   - Selenium/Puppeteer ì‚¬ìš©
   - JavaScript ë Œë”ë§ ëŒ€ê¸°

4. ë°ì´í„° ì¶”ì¶œ
   - ìƒê¸° ì„ íƒìë¡œ DOM íƒìƒ‰
   - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ

ì¥ì :
  - ëª¨ë“  ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ê°€ëŠ¥
  - ì „ì²´ ë‚´ìš© ì·¨ë“
  
ë‹¨ì :
  - ëŠë¦° ì²˜ë¦¬
  - ì‚¬ì´íŠ¸ ë³€ê²½ ì‹œ ìœ ì§€ë³´ìˆ˜
  - ë†’ì€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
```

### 4.3 ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§

```yaml
scheduler:
  default_interval_minutes: 30      # ê¸°ë³¸ ê°±ì‹  ê°„ê²©
  
  sources:
    "naver_news":
      interval_minutes: 15          # 15ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
      max_per_request: 50           # 1íšŒë‹¹ ìµœëŒ€ 50ê°œ
    
    "google_news":
      interval_minutes: 20
      max_per_request: 100
    
    "naver_blog":
      interval_minutes: 60
      max_per_request: 30
  
  # ì‹œê°„ëŒ€ë³„ ìˆ˜ì§‘ ê°•ë„
  time_slots:
    "09:00-11:00":                  # ì•„ì¹¨ í”¼í¬
      interval_minutes: 10
    "14:00-16:00":                  # ì˜¤í›„ í”¼í¬
      interval_minutes: 10
    "22:00-06:00":                  # ì•¼ê°„
      interval_minutes: 60          # ëŠìŠ¨í•˜ê²Œ
```

### 4.4 ì—ëŸ¬ ì²˜ë¦¬

```
ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ:
  1ì°¨: 5ì´ˆ í›„ ì¬ì‹œë„
  2ì°¨: 30ì´ˆ í›„ ì¬ì‹œë„
  3ì°¨: 300ì´ˆ(5ë¶„) í›„ ì¬ì‹œë„
  
ì‹¤íŒ¨ íšŸìˆ˜ 3íšŒ ì´ˆê³¼:
  - ì†ŒìŠ¤ë¥¼ ì¼ì‹œ ë¹„í™œì„±í™” (1ì‹œê°„)
  - ê´€ë¦¬ì ì•Œë¦¼ ë°œì†¡
  - ë¡œê·¸ ê¸°ë¡

íƒ€ì„ì•„ì›ƒ ì„¤ì •:
  - ì—°ê²°: 10ì´ˆ
  - ì½ê¸°: 30ì´ˆ
  - ì „ì²´: 60ì´ˆ
```

---

## 5ë‹¨ê³„: í•„í„°ë§ ë° ë­í‚¹ ì—”ì§„

### 5.1 í•„í„°ë§ (Filtering)

#### A. ê·œì¹™ ê¸°ë°˜ í•„í„° (Rule-based)

```
1. ë‚ ì§œ í•„í„°
   ì¡°ê±´: published_at >= ì‹œì‘ì¼ AND published_at <= ì¢…ë£Œì¼
   ì„¤ì •: min_days_old=0, max_days_old=30
   
   ë¡œì§:
   current_time = now()
   news_age_days = (current_time - published_at) / 86400
   pass = (news_age_days >= min_days_old) AND (news_age_days <= max_days_old)

2. í‚¤ì›Œë“œ í•„í„°
   ì¡°ê±´: (ì œëª© ë˜ëŠ” ë‚´ìš©) í¬í•¨ (í‚¤ì›Œë“œ)
   ì„¤ì •: keywords=[í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2], match_type=ANY (ë˜ëŠ” ALL)
   
   ë¡œì§:
   if match_type == "ANY":
     pass = any(keyword in title or keyword in content for keyword in keywords)
   elif match_type == "ALL":
     pass = all(keyword in title or keyword in content for keyword in keywords)

3. ì¹´í…Œê³ ë¦¬ í•„í„°
   ì¡°ê±´: category IN (ì„ íƒëœ ì¹´í…Œê³ ë¦¬ë“¤)
   ì„¤ì •: categories=["ì •ì¹˜", "ê²½ì œ"]
   
   ë¡œì§:
   pass = category in selected_categories

4. ë‚´ìš© ê¸¸ì´ í•„í„°
   ì¡°ê±´: min_length <= content_length <= max_length
   ì„¤ì •: min_content_length=300, max_content_length=50000
   
   ë¡œì§:
   pass = (len(content) >= min_length) AND (len(content) <= max_length)

5. ì‹ ë¢°ë„ í•„í„°
   ì¡°ê±´: credibility_score >= threshold
   ì„¤ì •: min_credibility=70
   
   ë¡œì§:
   source_credibility = get_source_credibility(source)
   pass = source_credibility >= min_credibility

6. ì¤‘ë³µ í•„í„°
   ì¡°ê±´: ë™ì¼í•œ ë‰´ìŠ¤ ì¤‘ë³µ ì œê±°
   ì„¤ì •: duplicate_check_method=semantic_hash
   
   ë¡œì§:
   hash = generate_hash(title + content)  # ê°„ë‹¨í•œ í•´ì‹œ
   pass = hash not in seen_hashes
   
   (AI ë¯¸ì‚¬ìš©: ìœ ì‚¬ë„ ê³„ì‚° ë¶ˆí•„ìš”, ì •í™•í•œ ì¤‘ë³µë§Œ ì œê±°)

7. ì œì™¸ í‚¤ì›Œë“œ í•„í„°
   ì¡°ê±´: (ì œëª© ë˜ëŠ” ë‚´ìš©) í¬í•¨í•˜ì§€ ì•ŠìŒ (ì œì™¸ í‚¤ì›Œë“œ)
   ì„¤ì •: exclude_keywords=["ê´‘ê³ ", "ìŠ¤í°ì„œ"]
   
   ë¡œì§:
   pass = not any(keyword in title or keyword in content for keyword in exclude_keywords)
```

#### B. í•„í„° ì ìš© ì•Œê³ ë¦¬ì¦˜

```python
def apply_filters(news_list, filter_config):
    """
    ëª¨ë“  í•„í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©
    """
    filtered_news = news_list
    
    # 1. ë‚ ì§œ í•„í„°
    if filter_config.date_range:
        filtered_news = [n for n in filtered_news if is_within_date_range(n, filter_config.date_range)]
    
    # 2. ì¹´í…Œê³ ë¦¬ í•„í„°
    if filter_config.categories:
        filtered_news = [n for n in filtered_news if n.category in filter_config.categories]
    
    # 3. í‚¤ì›Œë“œ í•„í„° (í¬í•¨)
    if filter_config.include_keywords:
        filtered_news = [n for n in filtered_news if matches_keywords(n, filter_config.include_keywords)]
    
    # 4. í‚¤ì›Œë“œ í•„í„° (ì œì™¸)
    if filter_config.exclude_keywords:
        filtered_news = [n for n in filtered_news if not matches_keywords(n, filter_config.exclude_keywords)]
    
    # 5. ë‚´ìš© ê¸¸ì´ í•„í„°
    if filter_config.content_length:
        filtered_news = [n for n in filtered_news if is_valid_length(n, filter_config.content_length)]
    
    # 6. ì‹ ë¢°ë„ í•„í„°
    if filter_config.min_credibility:
        filtered_news = [n for n in filtered_news if n.credibility_score >= filter_config.min_credibility]
    
    # 7. ì¤‘ë³µ ì œê±°
    if filter_config.remove_duplicates:
        filtered_news = remove_duplicates(filtered_news)
    
    return filtered_news
```

### 5.2 ë­í‚¹ (Ranking)

#### A. ì ìˆ˜ ê³„ì‚° ê³µì‹

```
ìµœì¢… ì ìˆ˜ (Final Score) = Î£(ë¶€ë¶„ ì ìˆ˜ Ã— ê°€ì¤‘ì¹˜)

1. ì‹ ì„ ë„ ì ìˆ˜ (Freshness Score)
   
   ê³µì‹: freshness_score = 100 * exp(-ln(2) * age_hours / half_life_hours)
   
   ì„¤ëª…:
   - age_hours: ë‰´ìŠ¤ ë‚˜ì´ (ì‹œê°„ ë‹¨ìœ„)
   - half_life_hours: ë°˜ê°ê¸° (ê¸°ë³¸ê°’: 24ì‹œê°„)
   - ì˜ˆ: 12ì‹œê°„ í›„ 50ì , 24ì‹œê°„ í›„ 25ì 
   
   ì„¤ì •ê°’:
   {
     "half_life_hours": 24,
     "min_score": 5,        # ìµœì†Œê°’ (ë§¤ìš° ì˜¤ë˜ëœ ë‰´ìŠ¤ë„ 5ì )
     "max_score": 100
   }

2. ì‹ ë¢°ë„ ì ìˆ˜ (Credibility Score)
   
   ê³µì‹: credibility_score = source_credibility_score + author_bonus + verification_bonus
   
   êµ¬ì„±:
   - ì¶œì²˜ ì‹ ë¢°ë„: ì„¤ì • íŒŒì¼ì—ì„œ ì •ì˜ (0~100)
   - ì €ì ë³´ë„ˆìŠ¤: ì´ì „ ì¢‹ì€ í‰ê°€ ì €ì +5~10ì 
   - ê²€ì¦ ë³´ë„ˆìŠ¤: ì—¬ëŸ¬ ì¶œì²˜ì—ì„œ ê°™ì€ ë‰´ìŠ¤ ë³´ë„ +10~15ì 
   
   ì„¤ì •ê°’:
   {
     "sources": {
       "ì¡°ì„ ì¼ë³´": 85,
       "ì¤‘ì•™ì¼ë³´": 85,
       ...
     },
     "author_bonus": 5,
     "verification_bonus": 10,
     "min_score": 0,
     "max_score": 100
   }

3. ì¸ê¸°ë„ ì ìˆ˜ (Popularity Score)
   
   ê³µì‹:
   popularity_score = 100 * [
     0.4 * (views / view_normalization) +
     0.35 * (shares / share_normalization) +
     0.25 * (comments / comment_normalization)
   ]
   
   ì •ê·œí™” (Normalization):
   - view_normalization: ìƒìœ„ 5%ì˜ ì¡°íšŒìˆ˜
   - share_normalization: ìƒìœ„ 5%ì˜ ê³µìœ ìˆ˜
   - comment_normalization: ìƒìœ„ 5%ì˜ ëŒ“ê¸€ìˆ˜
   - ê°’ì´ 1ì„ ì´ˆê³¼í•˜ë©´ 100ìœ¼ë¡œ ì œí•œ
   
   ì„¤ì •ê°’:
   {
     "weights": {
       "view": 0.4,
       "share": 0.35,
       "comment": 0.25
     },
     "normalization_method": "percentile",  # ë˜ëŠ” "average", "max"
     "percentile": 95,
     "min_score": 0,
     "max_score": 100
   }

4. ì œëª©-ë‚´ìš© ì¼ì¹˜ë„ (Title-Content Match Score)
   
   ê³µì‹:
   match_score = (title_words_in_content_count / title_words_count) * 100
   
   ì„¤ëª…:
   - ì œëª©ì˜ ì£¼ìš” ë‹¨ì–´ê°€ ë³¸ë¬¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€ ì¸¡ì •
   - ë¶ˆìš©ì–´ ì œì™¸ (the, a, and, ...)
   - ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
   - ê¸¸ì´ 3ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ê³ ë ¤
   
   ì˜ˆ:
   ì œëª©: "ì‚¼ì„±ì „ì ìƒˆë¡œìš´ AI ì¹© ë°œí‘œ"
   ë‚´ìš©: "ì‚¼ì„±ì´ ì¸ê³µì§€ëŠ¥ ì¹©ì„ ë°œí‘œí–ˆë‹¤. ì´ AI ì¹©ì€..."
   
   ì œëª© ì£¼ìš” ë‹¨ì–´: ["ì‚¼ì„±ì „ì", "ìƒˆë¡œìš´", "AI", "ì¹©", "ë°œí‘œ"]
   ë‚´ìš© í¬í•¨ ë‹¨ì–´: ["ì‚¼ì„±", "AI", "ì¹©", "ë°œí‘œ"] (4ê°œ)
   ì¼ì¹˜ë„: 4/5 * 100 = 80ì 
   
   ì„¤ì •ê°’:
   {
     "min_word_length": 3,
     "stopwords": ["the", "a", "and", ...],
     "min_score": 10,  # ìµœì†Œê°’
     "max_score": 100
   }

5. ìµœì¢… ì ìˆ˜ (Final Score)
   
   ê³µì‹:
   final_score = min(100, max(0,
     freshness_score * w_freshness +
     credibility_score * w_credibility +
     popularity_score * w_popularity +
     match_score * w_match +
     bonus_scores
   ))
   
   ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì„¤ì • ê°€ëŠ¥):
   - w_freshness: 0.25
   - w_credibility: 0.30
   - w_popularity: 0.25
   - w_match: 0.20
   
   ë³´ë„ˆìŠ¤ ì ìˆ˜:
   - ê³µì‹ ë°œí‘œ/ë³´ë„ìë£Œ: +10ì 
   - ì‚¬ì§„/ë™ì˜ìƒ í¬í•¨: +5ì 
   - ê²€ì¦ëœ ì €ì: +5ì 
```

#### B. ë­í‚¹ ì•Œê³ ë¦¬ì¦˜

```python
def calculate_scores(news_list, config):
    """
    ëª¨ë“  ë‰´ìŠ¤ì— ì ìˆ˜ ê³„ì‚°
    """
    for news in news_list:
        news.scores = {
            "freshness": calculate_freshness_score(news, config.freshness_config),
            "credibility": calculate_credibility_score(news, config.credibility_config),
            "popularity": calculate_popularity_score(news, config.popularity_config),
            "title_content_match": calculate_match_score(news, config.match_config),
        }
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        weights = config.scoring.weights
        news.quality_score = (
            news.scores["freshness"] * weights.get("freshness", 0.25) +
            news.scores["credibility"] * weights.get("credibility", 0.30) +
            news.scores["popularity"] * weights.get("popularity", 0.25) +
            news.scores["title_content_match"] * weights.get("title_content_match", 0.20)
        )
        
        # ë³´ë„ˆìŠ¤ ì ìˆ˜ ì¶”ê°€
        news.quality_score += apply_bonuses(news, config.bonus_config)
        
        # 0~100 ë²”ìœ„ë¡œ ì •ê·œí™”
        news.quality_score = max(0, min(100, news.quality_score))
    
    return news_list

def rank_news(news_list, sort_by="quality_score", order="descending"):
    """
    ì ìˆ˜ì— ë”°ë¼ ì •ë ¬
    """
    reverse = (order == "descending")
    return sorted(news_list, key=lambda n: getattr(n, sort_by), reverse=reverse)
```

#### C. ìœ ì—°í•œ ë­í‚¹ ì„¤ì •

ì‚¬ìš©ìê°€ ëŸ°íƒ€ì„ì— ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ì •ë ¬ ê¸°ì¤€ì„ ë°”ê¿€ ìˆ˜ ìˆë„ë¡:

```yaml
ranking_presets:
  "ìµœì‹ ìˆœ":                         # í”„ë¦¬ì…‹ 1
    weights:
      freshness: 0.80
      credibility: 0.10
      popularity: 0.05
      title_content_match: 0.05
  
  "ì¸ê¸°ìˆœ":                         # í”„ë¦¬ì…‹ 2
    weights:
      freshness: 0.10
      credibility: 0.20
      popularity: 0.65
      title_content_match: 0.05
  
  "ì‹ ë¢°ë„ìˆœ":                       # í”„ë¦¬ì…‹ 3
    weights:
      freshness: 0.10
      credibility: 0.70
      popularity: 0.10
      title_content_match: 0.10
  
  "ê· í˜•ì¡íŒ":                       # í”„ë¦¬ì…‹ 4 (ê¸°ë³¸ê°’)
    weights:
      freshness: 0.25
      credibility: 0.30
      popularity: 0.25
      title_content_match: 0.20
```

---

## 6ë‹¨ê³„: API ì„¤ê³„

### 6.1 REST API ì—”ë“œí¬ì¸íŠ¸

#### A. ë‰´ìŠ¤ ê²€ìƒ‰ API

```
GET /api/v1/news/search

ìš”ì²­:
{
  "q": "string",                    // ê²€ìƒ‰ í‚¤ì›Œë“œ (ì„ íƒ)
  "category": "string",             // ì¹´í…Œê³ ë¦¬ (ì„ íƒ)
  "date_from": "YYYY-MM-DD",       // ê²€ìƒ‰ ì‹œì‘ì¼ (ì„ íƒ)
  "date_to": "YYYY-MM-DD",         // ê²€ìƒ‰ ì¢…ë£Œì¼ (ì„ íƒ)
  "sort_by": "quality_score",      // ì •ë ¬ ê¸°ì¤€ (ì„ íƒ)
  "sort_order": "descending",      // ì •ë ¬ ìˆœì„œ (ì„ íƒ)
  "limit": 20,                      // ê²°ê³¼ ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸: 20)
  "offset": 0                       // í˜ì´ì§• ì˜¤í”„ì…‹ (ì„ íƒ, ê¸°ë³¸: 0)
}

ì‘ë‹µ:
{
  "status": "success",
  "data": {
    "total": 1234,                  // ì „ì²´ ê²°ê³¼ ìˆ˜
    "limit": 20,
    "offset": 0,
    "news": [
      {
        "id": "news_123",
        "title": "...",
        "content": "...",
        "summary": "...",
        "source": "ì¡°ì„ ì¼ë³´",
        "url": "...",
        "category": "ê²½ì œ",
        "published_at": "2026-02-05T10:30:00Z",
        "quality_score": 87.5,
        "scores": {
          "freshness": 95,
          "credibility": 85,
          "popularity": 92,
          "title_content_match": 78
        }
      },
      ...
    ]
  },
  "timestamp": "2026-02-05T15:00:00Z"
}
```

#### B. íŠ¹ì • ë‚ ì§œ ì¸ê¸° ë‰´ìŠ¤ API

```
GET /api/v1/news/trending

ìš”ì²­:
{
  "date": "YYYY-MM-DD",            // íŠ¹ì • ë‚ ì§œ (ì„ íƒ, ê¸°ë³¸: ì˜¤ëŠ˜)
  "category": "string",             // ì¹´í…Œê³ ë¦¬ (ì„ íƒ)
  "limit": 10                       // ìƒìœ„ Nê°œ (ì„ íƒ, ê¸°ë³¸: 10)
}

ì‘ë‹µ:
{
  "status": "success",
  "data": {
    "date": "2026-02-05",
    "news": [
      {
        "id": "news_456",
        "title": "...",
        "source": "ì¤‘ì•™ì¼ë³´",
        "url": "...",
        "quality_score": 94.2,
        "popularity_score": 98
      },
      ...
    ]
  }
}
```

#### C. ë§ì´ ë³¸ ë‰´ìŠ¤ API

```
GET /api/v1/news/popular

ìš”ì²­:
{
  "period": "24h",                  // ê¸°ê°„: 24h, 7d, 30d (ì„ íƒ, ê¸°ë³¸: 24h)
  "category": "string",             // ì¹´í…Œê³ ë¦¬ (ì„ íƒ)
  "limit": 10                       // ìƒìœ„ Nê°œ (ì„ íƒ)
}

ì‘ë‹µ:
{
  "status": "success",
  "data": {
    "period": "24h",
    "news": [
      {
        "id": "news_789",
        "title": "...",
        "views": 50000,
        "shares": 2000,
        "comments": 5000,
        "quality_score": 88.5
      },
      ...
    ]
  }
}
```

#### D. ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ API

```
GET /api/v1/news/category/{category}

ìš”ì²­:
{
  "date_from": "YYYY-MM-DD",       // ê¸°ê°„ í•„í„° (ì„ íƒ)
  "date_to": "YYYY-MM-DD",
  "limit": 20,
  "offset": 0
}

ì‘ë‹µ:
{
  "status": "success",
  "data": {
    "category": "ê²½ì œ",
    "total": 3456,
    "news": [...]
  }
}
```

#### E. ì»¤ìŠ¤í…€ í•„í„° ê²€ìƒ‰ API

```
POST /api/v1/news/filter

ìš”ì²­ ë°”ë””:
{
  "filters": {
    "date_range": {
      "from": "2026-02-01",
      "to": "2026-02-05"
    },
    "keywords": {
      "include": ["AI", "ê¸°ìˆ "],
      "exclude": ["ê°€ì§œ"],
      "match_type": "ANY"
    },
    "categories": ["IT", "ê³¼í•™"],
    "content_length": {
      "min": 300,
      "max": 50000
    },
    "min_credibility": 70,
    "remove_duplicates": true
  },
  "ranking": {
    "preset": "balanced"  // ë˜ëŠ” custom weights
  },
  "limit": 20,
  "offset": 0
}

ì‘ë‹µ:
{
  "status": "success",
  "data": {
    "filters_applied": {...},
    "total": 567,
    "news": [...]
  }
}
```

### 6.2 CLI ì¸í„°í˜ì´ìŠ¤

```bash
# ê¸°ë³¸ ê²€ìƒ‰
python news_collector.py search --keyword "AI" --category "IT"

# ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰
python news_collector.py search --from 2026-02-01 --to 2026-02-05 --limit 50

# íŠ¹ì • ë‚ ì§œ ì¸ê¸° ë‰´ìŠ¤
python news_collector.py trending --date 2026-02-05 --limit 20

# ìµœê·¼ 24ì‹œê°„ ë§ì´ ë³¸ ë‰´ìŠ¤
python news_collector.py popular --period 24h --limit 30

# ì»¤ìŠ¤í…€ í•„í„° ê²€ìƒ‰
python news_collector.py filter --config custom_filter.yaml

# ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤
python news_collector.py category IT --limit 20

# ì„¤ì • íŒŒì¼ ê²€ì¦
python news_collector.py validate --config config.yaml

# ë‰´ìŠ¤ ìˆ˜ì§‘ ìˆ˜ë™ ì‹¤í–‰
python news_collector.py crawl --source naver_news

# ìºì‹œ ì´ˆê¸°í™”
python news_collector.py cache --clear
```

---

## 7ë‹¨ê³„: êµ¬í˜„ ì „ëµ ë° ìˆœì„œ

### 7.1 êµ¬í˜„ í˜ì´ì¦ˆ ë¶„ë¥˜

#### Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ (1-2ì£¼)
**ëª©í‘œ**: í•µì‹¬ ë°ì´í„° êµ¬ì¡° ë° ì €ì¥ì†Œ êµ¬ì¶•

1. **ë°ì´í„° ëª¨ë¸ ì •ì˜**
   - News, Source, Category ì—”í‹°í‹° í´ë˜ìŠ¤ ì •ì˜
   - SQLAlchemy ë˜ëŠ” Pydantic ëª¨ë¸ ì‘ì„±

2. **ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •**
   - SQLite ì´ˆê¸°í™” (ë˜ëŠ” PostgreSQL)
   - í…Œì´ë¸” ìƒì„±, ì¸ë±ì‹±
   - ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

3. **ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ**
   - YAML íŒŒì„œ êµ¬í˜„
   - ConfigManager í´ë˜ìŠ¤ ê°œë°œ
   - í™˜ê²½ ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›

4. **ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§**
   - ë¡œê±° ì„¤ì • (Python logging)
   - ì—ëŸ¬ ì¶”ì 
   - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**ì‚°ì¶œë¬¼**:
- `models/` ë””ë ‰í† ë¦¬: ë°ì´í„° ëª¨ë¸
- `config.yaml`: ê¸°ë³¸ ì„¤ì • íŒŒì¼
- `database/` ë””ë ‰í† ë¦¬: DB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸

---

#### Phase 2: ë‰´ìŠ¤ ìˆ˜ì§‘ (2-3ì£¼)
**ëª©í‘œ**: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ê°€ëŠ¥í•˜ê²Œ êµ¬í˜„

1. **Crawler ëª¨ë“ˆ**
   - BaseCrawler ì¶”ìƒ í´ë˜ìŠ¤ ì‘ì„±
   - APICrawler êµ¬í˜„ (Naver, Google News ë“±)
   - RSSCrawler êµ¬í˜„
   - WebCrawler êµ¬í˜„ (ì„ íƒì‚¬í•­, ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤)

2. **Parser ëª¨ë“ˆ**
   - NewsParser í´ë˜ìŠ¤ ì‘ì„±
   - HTML/JSON íŒŒì‹± ë¡œì§
   - í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
   - ì‹œê°„ í¬ë§· í†µì¼

3. **Validator ëª¨ë“ˆ**
   - í•„ìˆ˜ í•„ë“œ ê²€ì¦
   - ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜)
   - ê¸¸ì´ ê²€ì¦
   - URL ìœ íš¨ì„± í™•ì¸

4. **Scheduler êµ¬í˜„**
   - APScheduler ë˜ëŠ” ì‹œìŠ¤í…œ í¬ë¡  í™œìš©
   - ì†ŒìŠ¤ë³„ ê°±ì‹  ê°„ê²© ì„¤ì •
   - ì‹œê°„ëŒ€ë³„ ê°•ë„ ì¡°ì ˆ
   - ì—ëŸ¬ ì¬ì‹œë„ ë¡œì§

**ì‚°ì¶œë¬¼**:
- `crawlers/` ë””ë ‰í† ë¦¬: Crawler í´ë˜ìŠ¤ë“¤
- `parsers/` ë””ë ‰í† ë¦¬: Parser ë¡œì§
- `validators/` ë””ë ‰í† ë¦¬: ê²€ì¦ ë¡œì§
- `scheduler/` ë””ë ‰í† ë¦¬: ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

---

#### Phase 3: í•„í„°ë§ ë° ê²€ìƒ‰ (2-3ì£¼)
**ëª©í‘œ**: ê°•ë ¥í•œ ê²€ìƒ‰ ë° í•„í„°ë§ ê¸°ëŠ¥ êµ¬í˜„

1. **ì¸ë±ì‹± ì—”ì§„**
   - SQLite FTS (Full-Text Search) ë˜ëŠ” Elasticsearch
   - ì œëª©, ë‚´ìš©, ì¹´í…Œê³ ë¦¬ ì¸ë±ì‹±
   - ì£¼ê¸°ì  ì¸ë±ìŠ¤ ê°±ì‹ 

2. **í•„í„° ì—”ì§„**
   - FilterEngine í´ë˜ìŠ¤ êµ¬í˜„
   - ë‚ ì§œ, í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ê¸¸ì´ ë“± í•„í„°
   - ë³µí•© í•„í„° ì¡°í•© ì§€ì›
   - í•„í„° ë¡œì§ í…ŒìŠ¤íŠ¸

3. **ê²€ìƒ‰ ì—”ì§„**
   - SearchEngine í´ë˜ìŠ¤
   - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
   - í˜ì´ì§• ì§€ì›
   - ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” (ìºì‹±)

4. **ìºì‹±**
   - Redis ë˜ëŠ” ì¸ë©”ëª¨ë¦¬ ìºì‹œ
   - ìì£¼ ê²€ìƒ‰í•˜ëŠ” ì¿¼ë¦¬ ìºì‹±
   - TTL ì„¤ì •

**ì‚°ì¶œë¬¼**:
- `search/` ë””ë ‰í† ë¦¬: ê²€ìƒ‰ ì—”ì§„
- `filters/` ë””ë ‰í† ë¦¬: í•„í„° ë¡œì§
- `cache/` ë””ë ‰í† ë¦¬: ìºì‹± êµ¬í˜„
- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

---

#### Phase 4: ë­í‚¹ ì—”ì§„ (2ì£¼)
**ëª©í‘œ**: ë‹¤ì–‘í•œ ì ìˆ˜ ê¸°ë°˜ ë‰´ìŠ¤ ìˆœìœ„ ë§¤ê¸°ê¸°

1. **ì ìˆ˜ ê³„ì‚°**
   - ScoreCalculator í´ë˜ìŠ¤
   - ì‹ ì„ ë„, ì‹ ë¢°ë„, ì¸ê¸°ë„, ì¼ì¹˜ë„ ì ìˆ˜ ê³„ì‚°
   - ê³µì‹ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
   - í†µê³„ ì •ê·œí™” ë¡œì§

2. **ë­í‚¹ ì•Œê³ ë¦¬ì¦˜**
   - RankingEngine í´ë˜ìŠ¤
   - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìµœì¢… ì ìˆ˜ ê³„ì‚°
   - ë³´ë„ˆìŠ¤ ì ìˆ˜ ì ìš©
   - ë‹¤ì–‘í•œ ì •ë ¬ ì˜µì…˜

3. **ì„¤ì • ê´€ë¦¬**
   - ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •
   - í”„ë¦¬ì…‹ ê´€ë¦¬
   - ì‚¬ìš©ì ì»¤ìŠ¤í…€ ì„¤ì • ì§€ì›

**ì‚°ì¶œë¬¼**:
- `ranking/` ë””ë ‰í† ë¦¬: ë­í‚¹ ì—”ì§„
- ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

---

#### Phase 5: API ë° CLI (1-2ì£¼)
**ëª©í‘œ**: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì œê³µ

1. **REST API êµ¬í˜„**
   - Flask/FastAPI í”„ë ˆì„ì›Œí¬ ì„ íƒ
   - ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
   - ìš”ì²­/ì‘ë‹µ ê²€ì¦
   - ì—ëŸ¬ ì²˜ë¦¬

2. **CLI ë„êµ¬ ê°œë°œ**
   - Click ë˜ëŠ” argparse ì‚¬ìš©
   - ëª…ë ¹ì–´ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
   - ë„ì›€ë§ ì‘ì„±

3. **ë¬¸ì„œí™”**
   - API ë¬¸ì„œ (Swagger/OpenAPI)
   - CLI ì‚¬ìš© ì„¤ëª…ì„œ
   - ì„¤ì • íŒŒì¼ ì˜ˆì œ

**ì‚°ì¶œë¬¼**:
- `api/` ë””ë ‰í† ë¦¬: API êµ¬í˜„
- `cli/` ë””ë ‰í† ë¦¬: CLI ë„êµ¬
- `docs/` ë””ë ‰í† ë¦¬: ë¬¸ì„œ

---

#### Phase 6: í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (2ì£¼)
**ëª©í‘œ**: ì•ˆì •ì„± ë° ì„±ëŠ¥ ë³´ì¥

1. **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
   - ê° ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸ (pytest)
   - ë¼ì¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ

2. **í†µí•© í…ŒìŠ¤íŠ¸**
   - End-to-End íë¦„ í…ŒìŠ¤íŠ¸
   - ë°ì´í„°ë² ì´ìŠ¤ íŠ¸ëœì­ì…˜ í…ŒìŠ¤íŠ¸

3. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**
   - 1000ê°œ ë‰´ìŠ¤ ê²€ìƒ‰ < 1ì´ˆ
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
   - ìºì‹œ íš¨ìœ¨ì„± ë¶„ì„

4. **ì½”ë“œ ìµœì í™”**
   - ë³‘ëª© ì§€ì  í”„ë¡œíŒŒì¼ë§
   - ì¿¼ë¦¬ ìµœì í™”
   - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸

**ì‚°ì¶œë¬¼**:
- `tests/` ë””ë ‰í† ë¦¬: í…ŒìŠ¤íŠ¸ ì½”ë“œ
- ì„±ëŠ¥ ë³´ê³ ì„œ

---

### 7.2 êµ¬í˜„ ìŠ¤íƒ ì¶”ì²œ

| ì˜ì—­ | ê¸°ìˆ  | ì´ìœ  |
|------|------|------|
| **ì–¸ì–´** | Python 3.9+ | ë°ì´í„° ì²˜ë¦¬, ë¹ ë¥¸ ê°œë°œ |
| **ì›¹ í”„ë ˆì„ì›Œí¬** | FastAPI | ë†’ì€ ì„±ëŠ¥, ìë™ ê²€ì¦, ë¬¸ì„œí™” |
| **ORM** | SQLAlchemy | ìœ ì—°ì„±, ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì› |
| **ë°ì´í„°ë² ì´ìŠ¤** | PostgreSQL (ë˜ëŠ” SQLite) | ì•ˆì •ì„±, FTS ì§€ì› |
| **ìºì‹±** | Redis | ë¹ ë¥¸ ì¡°íšŒ, ì„¸ì…˜ ê´€ë¦¬ |
| **ê²€ìƒ‰** | Elasticsearch (ì„ íƒ) | ë³µì¡í•œ ê²€ìƒ‰ ìš”êµ¬ì‚¬í•­ ì‹œ |
| **ìŠ¤ì¼€ì¤„ëŸ¬** | APScheduler | ìœ ì—°í•œ ìŠ¤ì¼€ì¤„ë§ |
| **HTTP í´ë¼ì´ì–¸íŠ¸** | httpx ë˜ëŠ” requests | ë¹„ë™ê¸° ì§€ì› |
| **ë¡œê¹…** | Python logging | í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| **í…ŒìŠ¤íŠ¸** | pytest | ê°•ë ¥í•œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ |
| **ë¬¸ì„œ** | Sphinx + OpenAPI | ìë™ ë¬¸ì„œí™” |

### 7.3 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
news_collector/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.yaml                 # ë©”ì¸ ì„¤ì •
â”‚   â”œâ”€â”€ logging_config.yaml         # ë¡œê¹… ì„¤ì •
â”‚   â””â”€â”€ sources.yaml                # ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news.py                     # News ì—”í‹°í‹°
â”‚   â”œâ”€â”€ source.py                   # Source ì—”í‹°í‹°
â”‚   â””â”€â”€ category.py                 # Category ì—”í‹°í‹°
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db.py                       # DB ì—°ê²°
â”‚   â”œâ”€â”€ init.py                     # ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ migrations/                 # Alembic ë§ˆì´ê·¸ë ˆì´ì…˜
â”‚
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                     # BaseCrawler
â”‚   â”œâ”€â”€ api_crawler.py              # API ê¸°ë°˜ ìˆ˜ì§‘
â”‚   â”œâ”€â”€ rss_crawler.py              # RSS ê¸°ë°˜ ìˆ˜ì§‘
â”‚   â””â”€â”€ web_crawler.py              # ì›¹ í¬ë¡¤ë§ (ì„ íƒ)
â”‚
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news_parser.py              # ë‰´ìŠ¤ íŒŒì‹±
â”‚   â””â”€â”€ html_parser.py              # HTML íŒŒì‹± ìœ í‹¸
â”‚
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ news_validator.py           # ë‰´ìŠ¤ ê²€ì¦
â”‚
â”œâ”€â”€ search/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_engine.py            # ê²€ìƒ‰ ì—”ì§„
â”‚   â””â”€â”€ indexer.py                  # ì¸ë±ì‹±
â”‚
â”œâ”€â”€ filters/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ filter_engine.py            # í•„í„° ì—”ì§„
â”‚
â”œâ”€â”€ ranking/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scorer.py                   # ì ìˆ˜ ê³„ì‚°
â”‚   â””â”€â”€ ranker.py                   # ë­í‚¹ ì—”ì§„
â”‚
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ task_scheduler.py           # ìŠ¤ì¼€ì¤„ë§
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                     # FastAPI ì•±
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ news.py                 # /api/news ì—”ë“œí¬ì¸íŠ¸
â”‚       â””â”€â”€ search.py               # /api/search ì—”ë“œí¬ì¸íŠ¸
â”‚
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ cli.py                      # CLI ëª…ë ¹ì–´
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_manager.py           # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ logger.py                   # ë¡œê¹… ìœ í‹¸
â”‚   â””â”€â”€ helpers.py                  # í—¬í¼ í•¨ìˆ˜
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_crawlers.py            # Crawler í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_filters.py             # í•„í„° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_ranking.py             # ë­í‚¹ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_search.py              # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md                      # API ë¬¸ì„œ
â”‚   â”œâ”€â”€ CLI.md                      # CLI ì‚¬ìš© ì„¤ëª…ì„œ
â”‚   â””â”€â”€ CONFIGURATION.md            # ì„¤ì • ê°€ì´ë“œ
â”‚
â”œâ”€â”€ requirements.txt                # íŒŒì´ì¬ ì˜ì¡´ì„±
â”œâ”€â”€ pyproject.toml                  # í”„ë¡œì íŠ¸ ì„¤ì •
â”œâ”€â”€ pytest.ini                      # Pytest ì„¤ì •
â”œâ”€â”€ README.md                       # í”„ë¡œì íŠ¸ ê°œìš”
â””â”€â”€ main.py                         # ì§„ì…ì 
```

### 7.4 ë‹¨ê³„ë³„ ë§ˆì¼ìŠ¤í†¤

| ì£¼ì°¨ | ëª©í‘œ | ì‚°ì¶œë¬¼ |
|------|------|--------|
| 1-2ì£¼ | Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ | ë°ì´í„° ëª¨ë¸, DB, ì„¤ì • ì‹œìŠ¤í…œ |
| 3-5ì£¼ | Phase 2: ë‰´ìŠ¤ ìˆ˜ì§‘ | Crawler, Parser, Validator |
| 6-8ì£¼ | Phase 3: ê²€ìƒ‰ ë° í•„í„° | ê²€ìƒ‰ ì—”ì§„, í•„í„° ë¡œì§ |
| 9-10ì£¼ | Phase 4: ë­í‚¹ ì—”ì§„ | ì ìˆ˜ ê³„ì‚°, ë­í‚¹ ë¡œì§ |
| 11-12ì£¼ | Phase 5: API & CLI | REST API, CLI ë„êµ¬ |
| 13-14ì£¼ | Phase 6: í…ŒìŠ¤íŠ¸ ìµœì í™” | í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80%+, ì„±ëŠ¥ ìµœì í™” |

---

## 8. AI ìµœì†Œí™” ë° í•˜ë“œì½”ë”© ì œê±° ì „ëµ

### 8.1 AI ì‚¬ìš© ìµœì†Œí™”

#### âŒ í”¼í•´ì•¼ í•  AI ê¸°ìˆ 
- **ê°ì • ë¶„ì„ (Sentiment Analysis)**: ë‰´ìŠ¤ ê°ì • íŒë‹¨ X
- **í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text Classification)**: ì‹ ê²½ë§ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ X
- **ìœ ì‚¬ë„ ì¸¡ì • (Semantic Similarity)**: ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ ì°¾ê¸° X
- **ìë™ ìš”ì•½ (Abstractive Summarization)**: ì¸ê³µì§€ëŠ¥ ìš”ì•½ X
- **ê°œì²´ëª… ì¸ì‹ (NER)**: ìë™ ê°œì²´ ì¶”ì¶œ X

#### âœ… ì‚¬ìš©í•´ë„ ë˜ëŠ” AI ê¸°ìˆ 
- **í˜•íƒœì†Œ ë¶„ì„ (Tokenization)**: ë‹¨ì–´ ë¶„í•  (ê°€ë²¼ìš´ ê·œì¹™ ê¸°ë°˜)
- **ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)**: ì •í•´ì§„ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
- **ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ê·œí™”**: ëŒ€ì†Œë¬¸ì í†µì¼, ê³µë°± ì œê±°

### 8.2 í•˜ë“œì½”ë”© ì œê±° ì „ëµ

#### âŒ í•˜ë“œì½”ë”© ì˜ˆì‹œ
```python
# ë‚˜ìœ ì˜ˆ: ì†ŒìŠ¤ ì‹ ë¢°ë„ê°€ ì½”ë“œì— ë°•í˜€ìˆìŒ
if source == "ì¡°ì„ ì¼ë³´":
    credibility = 85
elif source == "ì¤‘ì•™ì¼ë³´":
    credibility = 85
elif source == "ê²½í–¥ì‹ ë¬¸":
    credibility = 80
```

#### âœ… ì„¤ì • ê¸°ë°˜ ì˜ˆì‹œ
```python
# ì¢‹ì€ ì˜ˆ: config.yamlì—ì„œ ì½ìŒ
config = load_config("config.yaml")
credibility = config["sources"][source]["credibility"]
```

### 8.3 ì„¤ì • íŒŒì¼ ê¸°ë°˜ ìš´ì˜

ëª¨ë“  ë³€ê²½ ì‚¬í•­ì´ ì„¤ì • íŒŒì¼ì— ë°˜ì˜ë˜ì–´ì•¼ í•¨:

- âœ… ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ê°€/ì œê±°
- âœ… í•„í„° ì„ê³„ê°’ ì¡°ì •
- âœ… ì ìˆ˜ ê°€ì¤‘ì¹˜ ë³€ê²½
- âœ… ì¹´í…Œê³ ë¦¬ ìˆ˜ì •
- âœ… ìŠ¤ì¼€ì¤„ ë³€ê²½
- âŒ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”

---

## 9. ì„±ëŠ¥ ë° í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### 9.1 ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œ | êµ¬í˜„ ë°©ë²• |
|--------|------|---------|
| ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ | < 1ì´ˆ | ì¸ë±ì‹±, ìºì‹± |
| ë‰´ìŠ¤ ìˆ˜ì§‘ | ë¶„ë‹¹ 1000+ ê¸°ì‚¬ | ë³‘ë ¬ ì²˜ë¦¬, ë¹„ë™ê¸° |
| ë™ì‹œ ì‚¬ìš©ì | 100+ | ìŠ¤ì¼€ì¼ ì•„ì›ƒ ê°€ëŠ¥ êµ¬ì¡° |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | < 500MB (ê¸°ë³¸) | ìŠ¤íŠ¸ë¦¬ë°, í˜ì´ì§• |

### 9.2 í™•ì¥ ì „ëµ

#### ë‹¨ê³„ë³„ í™•ì¥

**ì´ˆê¸° (1ë‹¬)**
```
- SQLite ë¡œì»¬ DB
- ì¸ë©”ëª¨ë¦¬ ìºì‹œ
- ë‹¨ì¼ ì„œë²„
- 3-5ê°œ ë‰´ìŠ¤ ì†ŒìŠ¤
```

**ì„±ì¥ê¸° (3ê°œì›”)**
```
- PostgreSQLë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
- Redis ìºì‹œ ì¶”ê°€
- ë¡œë“œ ë°¸ëŸ°ì„œ ë„ì…
- 10ê°œ+ ë‰´ìŠ¤ ì†ŒìŠ¤
- ê²€ìƒ‰ ì¸ë±ìŠ¤ ë¶„ë¦¬
```

**ì„±ìˆ™ê¸° (6ê°œì›”+)**
```
- ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
- Elasticsearch ë„ì…
- ë‹¤ì¤‘ ë°ì´í„° ì„¼í„°
- 20ê°œ+ ë‰´ìŠ¤ ì†ŒìŠ¤
- ë©”ì‹œì§€ í ì¶”ê°€ (RabbitMQ/Kafka)
```

---

## ë¶€ë¡: ìš©ì–´ ì •ì˜

| ìš©ì–´ | ì •ì˜ |
|------|------|
| **ë‰´ìŠ¤ ì†ŒìŠ¤** | ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ëŒ€ìƒ (ì–¸ë¡ ì‚¬, API, RSS ë“±) |
| **í¬ë¡¤ë§** | ìë™ìœ¼ë¡œ ì›¹ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì • |
| **íŒŒì‹±** | ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜ |
| **ì¸ë±ì‹±** | ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´ ë°ì´í„°ì— ì¸ë±ìŠ¤ ë¶€ì—¬ |
| **í•„í„°ë§** | ì¡°ê±´ì— ë”°ë¼ ë‰´ìŠ¤ ì„ ë³„ |
| **ë­í‚¹** | ì ìˆ˜ì— ë”°ë¼ ë‰´ìŠ¤ ìˆœì„œ ë§¤ê¸°ê¸° |
| **ì‹ ì„ ë„** | ë‰´ìŠ¤ê°€ ìµœì‹ ì¸ ì •ë„ |
| **ì‹ ë¢°ë„** | ë‰´ìŠ¤ ì¶œì²˜ì˜ ì‹ ë¢°ì„± |
| **ì¸ê¸°ë„** | ì¡°íšŒìˆ˜, ê³µìœ ìˆ˜ ë“± |
| **ê°€ì¤‘ì¹˜** | ê° ì ìˆ˜ì˜ ì¤‘ìš”ë„ ë¹„ìœ¨ |

---

## ë§ˆì¹˜ë©°

ì´ ê¸°íš ë¬¸ì„œëŠ” **ë‹¨ê³„ë³„, ëª¨ë“ˆë³„ë¡œ êµ¬ì„±**ë˜ì–´ ìˆì–´ ê° ë‹¨ê³„ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**:
1. ğŸ”§ **ì„¤ì • ê¸°ë°˜ ìš´ì˜**: ì½”ë“œ ìˆ˜ì • ì—†ì´ ì„¤ì •ìœ¼ë¡œ ì¡°ì •
2. ğŸ¤– **AI ìµœì†Œí™”**: ê·œì¹™ ê¸°ë°˜, í†µê³„ ê¸°ë°˜ ì ‘ê·¼
3. ğŸ§© **ëª¨ë“ˆì‹ ì„¤ê³„**: ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘
4. ğŸ“Š **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ì†ŒìŠ¤, í•„í„°, ì ìˆ˜ ëª¨ë¸ ì¶”ê°€ ìš©ì´

ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ Phase 1ë¶€í„° ì°¨ê·¼ì°¨ê·¼ êµ¬í˜„í•˜ë©´ **ì™„ë²½í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ**ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
